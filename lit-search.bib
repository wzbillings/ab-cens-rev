@article{agarwal2012,
  title = {Statistics for {{Surgeons}} - {{Understanding Survival Analysis}}},
  author = {Agarwal, Girdhar Gopal},
  year = {2012},
  month = sep,
  journal = {India J. Surg. Oncol.},
  volume = {3},
  number = {3},
  pages = {208--214},
  publisher = {{Springer India}},
  address = {{New Delhi}},
  issn = {0975-7651, 0976-6952},
  doi = {10.1007/s13193-012-0149-z},
  urldate = {2023-10-30},
  abstract = {Survival data stand out as a special statistical field. This paper tries to describe what survival data is and what makes it so special. Survival data concerns times to some events. A key point is the successive observation of time, which on the one hand leads to sometimes not being observed so that all that is known is that they exceed some given times (censoring), and on the other hand implies that predictions regarding the future course should be conditional on the present status (truncation). In the simplest case, this condition is that the individual is alive. The successive conditioning makes the hazard function, which describes the probability of an event happening during a short interval given that the individual is alive today, the most relevant concept. Here we discuss parametric as well as non-parametric methods. Examples are presented in a way that can be followed without the help of computers.},
  langid = {english},
  keywords = {Censored data,Hazard function,Kaplan-Meier estimate,Logrank test,Survivor function},
  annotation = {Web of Science ID: WOS:000219024900010},
  file = {C:\Users\Zane\Zotero\storage\A8G2IWNM\Agarwal - 2012 - Statistics for Surgeons - Understanding Survival A.pdf}
}

@article{aragon1993,
  title = {Analysis of Disease Progression from Clinical Observations of United-States Air-Force Active Duty Members Infected with the Human-Immunodeficiency-Virus - Distribution of Aids Survival-Time from Interval-Censored Observations},
  author = {ARAGON, J and WESTON, M and WARNER, R},
  year = {1993},
  month = mar,
  journal = {VACCINE},
  volume = {11},
  number = {5},
  pages = {552--554},
  issn = {0264-410X},
  doi = {10.1016/0264-410X(93)90230-U},
  unique-id = {WOS:A1993KX41800016}
}

@article{arunajadai2012,
  title = {Handling Covariates Subject to Limits of Detection in Regression},
  author = {Arunajadai, Srikesh G. and Rauh, Virginia A.},
  year = {2012},
  month = sep,
  journal = {Environ Ecol Stat},
  volume = {19},
  number = {3},
  pages = {369--391},
  issn = {1573-3009},
  doi = {10.1007/s10651-012-0191-6},
  urldate = {2023-06-02},
  abstract = {In the environmental health sciences, measurements of toxic exposures are often constrained by a lower limit called the limit of detection (LOD), with observations below this limit called non-detects. Although valid inference may be obtained by excluding non-detects in the estimation of exposure effects, this practice can lead to substantial reduction in power to detect a significant effect, depending on the proportion of censoring and the closeness of the effect size to the null value. Therefore, a variety of methods have been commonly used in the environmental science literature to substitute values for the non-detects for the purpose of estimating exposure effects, including ad hoc values such as \$\$\{LOD/2, LOD/\textbackslash sqrt\{2\}\}\$\$and LOD. Another method substitutes the expected value of the non-detects, i.e., E[X|X {$\leq~$}LOD] but this requires that the inference be robust to mild miss-specifications in the distribution of the exposure variable. In this paper, we demonstrate that the estimate of the exposure effect is extremely sensitive to ad-hoc substitutions and moderate distribution miss-specifications under the conditions of large sample sizes and moderate effect size, potentially leading to biased estimates. We propose instead the use of the generalized gamma distribution to estimate imputed values for the non-detects, and show that this method avoids the risk of distribution miss-specification among the class of distributions represented by the generalized gamma distribution. A multiple imputation-based procedure is employed to estimate the regression parameters. Compared to the method of excluding non-detects, the proposed method can substantially increase the power to detect a significant effect when the effect size is close to the null value in small samples with moderate levels of censoring (~{$\leq$} 50\%), without compromising the coverage and relative bias of the estimates.},
  langid = {english},
  keywords = {Generalized gamma distribution,Limit of detection,Multiple imputation,Regression},
  file = {C:\Users\Zane\Zotero\storage\Z9WSID56\Arunajadai and Rauh - 2012 - Handling covariates subject to limits of detection.pdf}
}

@article{authority2010,
  title = {Management of Left-Censored Data in Dietary Exposure Assessment of Chemical Substances},
  author = {Authority, European Food Safety},
  year = {2010},
  journal = {EFSA Journal},
  volume = {8},
  number = {3},
  pages = {1557},
  issn = {1831-4732},
  doi = {10.2903/j.efsa.2010.1557},
  urldate = {2023-10-31},
  abstract = {Within the general framework of chemical risk assessment, a difficult step in dietary exposure assessment is the handling of concentration data reported to be below the limit of detection (LOD). These data are known as non-detects and the resulting distribution of occurrence values is left-censored. Handling left-censored data represents a challenge for EFSA?s collection and statistical analysis of chemical occurrence data. EFSA has so far treated left-censored data with widely used substitution methods recommended by international organisations. The appropriateness of this approach has a natural limitation in the computation of percentiles and in the application of statistical techniques. An EFSA working group was established to estimate the accuracy of methods currently used and to propose recommendations for more advanced alternative statistical approaches. Based on a simulation study and on analyses of real data, an ad hoc evaluation was carried out to assess the performance of different statistical methods to handle non-detects, i.e. parametric Maximum likelihood (ML) models, the log-probit regression method and the non-parametric Kaplan-Meier (KM) method. Results showed that the number of samples had a relatively limited impact on the accuracy and precision of estimates, but the degree of censoring had a large effect. When analysing a complex set of data, it was also shown that it is essential to identify possible sources of heterogeneity in a dataset, such as country of sample collection/origin, food group, laboratory, etc. Statistical analyses should either be conducted separately from these factors, or, to explicitly account for this heterogeneity, fixed/random effect ML models could be used. Based on a minimum number of available samples and to different values of censoring percentages, the working group outlined recommendations, including the use of appropriate statistical tests, to handle left-censored distributions of chemical contaminant data in the context of exposure assessment.},
  copyright = {\textcopyright{} 2010 European Food Safety Authority},
  langid = {english},
  keywords = {chemical contaminants,DATA SETS,detection limit,INFORMATION,laboratory sensitivity,Left-censored distribution,LEVEL,MODEL,non-detects,parametric methods,quantification limit,RISK-ASSESSMENT,WATER-QUALITY DATA},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\JX6LBJSD\\Authority - 2010 - Management of left-censored data in dietary exposu.pdf;C\:\\Users\\Zane\\Zotero\\storage\\33HD5NFF\\j.efsa.2010.html}
}

@article{baccarelli2005,
  title = {Handling of Dioxin Measurement Data in the Presence of Non-Detectable Values: {{Overview}} of Available Methods and Their Application in the {{Seveso}} Chloracne Study},
  shorttitle = {Handling of Dioxin Measurement Data in the Presence of Non-Detectable Values},
  author = {Baccarelli, A. and Pfeiffer, R. and Consonni, D. and Pesatori, A. C. and Bonzini, M. and Patterson, D. G. and Bertazzi, P. A. and Landi, M. T.},
  year = {2005},
  month = aug,
  journal = {Chemosphere},
  volume = {60},
  number = {7},
  pages = {898--906},
  publisher = {{Pergamon-Elsevier Science Ltd}},
  address = {{Oxford}},
  issn = {0045-6535},
  doi = {10.1016/j.chemosphere.2005.01.055},
  urldate = {2023-10-31},
  abstract = {Exposure measurements of concentrations that are non-detectable or near the detection limit (DL) are common in environmental research. Proper statistical treatment of non-detects is critical to avoid bias and unnecessary loss of information. In the present work, we present an overview of possible statistical strategies for handling non-detectable values, including deletion, simple substitution, distributional methods, and distribution-based imputation. Simple substitution methods (e.g., substituting 0, DL/2, DL/root 2, or DL for the non-detects) are the most commonly applied, even though the EPA Guidance for Data Quality Assessment discouraged their use when the percentage of non-detects is {$>$} 15\%. Distribution-based multiple imputation methods, also known as robust or "fill-in" procedures, may produce dependable results even when 50-70\% of the observations are non-detects and can be performed using commonly available statistical software. Any statistical analysis can be conducted on the imputed datasets. Results properly reflect the presence of non-detectable values and produce valid statistical inference. We describe the use of distribution-based multiple imputation in a recent investigation conducted on subjects from the Seveso population exposed to 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD), in which 55.6\% of plasma TCDD measurements were non-detects. We suggest that distribution-based multiple imputation be the preferred method to analyze environmental data when substantial proportions of observations are non-detects. (c) 2005 Elsevier Ltd. All rights reserved.},
  langid = {english},
  keywords = {{2,3,7,8-tetrachlorodibenzo-p-dioxin},detection limit,DETECTION LIMIT,ENVIRONMENTAL DATA SETS,EXPOSURE,exposure assessment,MASS-SPECTROMETRIC ANALYSIS,multiple imputation,non-detects,PHTHALATE,POPULATION,SAMPLES,SERUM,Seveso,SOIL,WATER},
  annotation = {Web of Science ID: WOS:000231300200008}
}

@article{beal2001,
  title = {Ways to {{Fit}} a {{PK Model}} with {{Some Data Below}} the {{Quantification Limit}}},
  author = {Beal, Stuart L},
  year = {2001},
  journal = {Journal of Pharmacokinetics and Pharmacodynamics},
  volume = {28},
  number = {5},
  pages = {481--504},
  langid = {english},
  keywords = {censored data,model fitting,quantification limit},
  file = {C:\Users\Zane\Zotero\storage\VYTTXW8E\beal01jpp.pdf}
}

@article{becker2001,
  title = {Advances in Medical Statistics Arising from the {{AIDS}} Epidemic},
  author = {Becker, N. G. and Marschner, I. C.},
  year = {2001},
  month = apr,
  journal = {Stat. Methods Med. Res.},
  volume = {10},
  number = {2},
  pages = {117--140},
  publisher = {{SAGE Publications Ltd}},
  address = {{London}},
  issn = {0962-2802, 1477-0334},
  doi = {10.1191/096228001667140468},
  urldate = {2023-10-30},
  abstract = {Many statisticians have contributed to studies of the HIV epidemic and progression to AIDS. They have developed new statistical methodology, where needed, to address HIV-related issues. The transfer of methods from one area to another often involves a substantial delay. This paper points to methods that were developed in the HIV context and have either already found applications in other areas of medical research or have the potential for such applications, with the hope that this will promote a speedier Transfer of the research methods. Among the new tools that HIV studies have placed firmly into the pool of statistical methods for medical research are the methods of back-calculation, methods for the analysis of retrospective ascertainment data and methods of analysis for the combined data from clinical trials and associated longitudinal studies. Notions that have been stimulated substantially are use of surrogate endpoints in clinical trials and screening blood products by the use of pooled serum samples. Research activity in many other areas has been boosted substantially through contributions motivated by HIV/AIDS studies. Noteworthy examples are analyses for doubly-censored lifetime data and methods for assessing vaccines for transmissible diseases.},
  langid = {english},
  keywords = {CD4 COUNTS,CLINICAL-TRIALS,DOUBLY CENSORED-DATA,DYNAMICS IN-VIVO,HIV-INFECTION RATES,HUMAN-IMMUNODEFICIENCY-VIRUS,LONGITUDINAL DATA,NEW-YORK-CITY,PROPORTIONAL HAZARDS MODEL,SURROGATE END-POINTS},
  annotation = {Web of Science ID: WOS:000168256200004}
}

@article{bergstrand2009,
  title = {Handling {{Data Below}} the {{Limit}} of {{Quantification}} in {{Mixed Effect Models}}},
  author = {Bergstrand, Martin and Karlsson, Mats O.},
  year = {2009},
  month = jun,
  journal = {AAPS J},
  volume = {11},
  number = {2},
  pages = {371--380},
  issn = {1550-7416},
  doi = {10.1208/s12248-009-9112-5},
  urldate = {2023-05-31},
  abstract = {The purpose of this study is to investigate the impact of observations below the limit of quantification (BQL) occurring in three distinctly different ways and assess the best method for prevention of bias in parameter estimates and for illustrating model fit using visual predictive checks (VPCs). Three typical ways in which BQL can occur in a model was investigated with simulations from three different models and different levels of the limit of quantification (LOQ). Model A was used to represent a case with BQL observations in an absorption phase of a PK model whereas model B represented a case with BQL observations in the elimination phase. The third model, C, an indirect response model illustrated a case where the variable of interest in some cases decreases below the LOQ before returning towards baseline. Different approaches for handling of BQL data were compared with estimation of the full dataset for 100 simulated datasets following models A, B, and C. An improved standard for VPCs was suggested to better evaluate simulation properties both for data above and below LOQ. Omission of BQL data was associated with substantial bias in parameter estimates for all tested models even for seemingly small amounts of censored data. Best performance was seen when the likelihood of being below LOQ was incorporated into the model. In the tested examples this method generated overall unbiased parameter estimates. Results following substitution of BQL observations with LOQ/2 were in some cases shown to introduce bias and were always suboptimal to the best method. The new standard VPCs was found to identify model misfit more clearly than VPCs of data above LOQ only.},
  langid = {english},
  keywords = {Age groups,Antibodies,{Antibodies, Viral},antibody titers,Antiviral Agents,association,Biomarkers,Chloroquine,Clinical Trials as Topic,Computer Simulation,Fold increase,geometric mean concentration,H1N1,Humans,Influenza,{Influenza A Virus, H1N1 Subtype},Influenza viruses,{Influenza, Human},Interval censored,left-censored data,Linear mixed-effects model,Linear Models,maximum likelihood inference,{Models, Statistical},Monte Carlo Method,Odds Ratio,Predictive Value of Tests,Risk ratio,Seasons,Serologic Tests,Serology,Software,Spring,Treatment Outcome},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\3ZIYFMUF\\Lindsey and Ryan - 1998 - Methods for interval-censored data.pdf;C\:\\Users\\Zane\\Zotero\\storage\\8M9Q6QH8\\Cauchemez et al. - 2012 - Influenza Infection Rates, Measurement Errors and .pdf;C\:\\Users\\Zane\\Zotero\\storage\\A5IPH6RR\\lindsey98sim - Methods for interval‐censored data.pdf;C\:\\Users\\Zane\\Zotero\\storage\\APSKET8K\\Xu et al. - 2015 - A Simple and Powerful Method for the Estimation of.pdf;C\:\\Users\\Zane\\Zotero\\storage\\C4E4INQS\\Irby et al. - 2021 - Approaches to handling missing or “problematic” ph.pdf;C\:\\Users\\Zane\\Zotero\\storage\\DT2TZQ8X\\Zelner et al. - 2019 - Effects of Sequential Influenza A(H1N1)pdm09 Vacci.pdf;C\:\\Users\\Zane\\Zotero\\storage\\JNXBRJPN\\Irby et al. - 2021 - Approaches to handling missing or “problematic” ph.pdf;C\:\\Users\\Zane\\Zotero\\storage\\JQFVSRGX\\Cauchemez et al. - 2012 - Influenza Infection Rates, Measurement Errors and .pdf;C\:\\Users\\Zane\\Zotero\\storage\\LGRE54EG\\Zelner et al. - 2019 - Effects of Sequential Influenza A(H1N1)pdm09 Vacci.pdf;C\:\\Users\\Zane\\Zotero\\storage\\N4BDU7ZN\\Gómez et al. - 2009 - Tutorial on methods for interval-censored data and.pdf;C\:\\Users\\Zane\\Zotero\\storage\\Q3BUC4JD\\Tran et al. - 2021 - Measuring association among censored antibody tite.pdf;C\:\\Users\\Zane\\Zotero\\storage\\R4LP8PPH\\tran21sim - Measuring association among censored antibody titer data.pdf;C\:\\Users\\Zane\\Zotero\\storage\\TIFGPDCZ\\bergstrand09aaps - Handling Data Below the Limit of Quantification in Mixed Effect Models.pdf;C\:\\Users\\Zane\\Zotero\\storage\\W47VUQQD\\Gómez et al. - 2009 - Tutorial on methods for interval-censored data and.pdf;C\:\\Users\\Zane\\Zotero\\storage\\NZI7WEG5\\5306487.html;C\:\\Users\\Zane\\Zotero\\storage\\Q3YXTLHB\\(SICI)1097-0258(19980130)172219AID-SIM7353.0.html;C\:\\Users\\Zane\\Zotero\\storage\\Q83INNUK\\psp4.html;C\:\\Users\\Zane\\Zotero\\storage\\VP59FNHD\\psp4.html}
}

@article{bernhardt2015,
  title = {Statistical {{Methods}} for {{Generalized Linear Models}} with {{Covariates Subject}} to {{Detection Limits}}},
  author = {Bernhardt, Paul W. and Wang, Huixia J. and Zhang, Daowen},
  year = {2015},
  month = may,
  journal = {Stat Biosci},
  volume = {7},
  number = {1},
  pages = {68--89},
  issn = {1867-1764, 1867-1772},
  doi = {10.1007/s12561-013-9099-4},
  urldate = {2023-05-31},
  abstract = {Censored observations are a common occurrence in biomedical data sets. Although a large amount of research has been devoted to estimation and inference for data with censored responses, very little research has focused on proper statistical procedures when predictors are censored. In this paper, we consider statistical methods for dealing with multiple predictors subject to detection limits within the context of generalized linear models. We investigate and adapt several conventional methods and develop a new multiple imputation approach for analyzing data sets with predictors censored due to detection limits. We establish the consistency and asymptotic normality of the proposed multiple imputation estimator and suggest a computationally simple and consistent variance estimator. We also demonstrate that the conditional mean imputation method often leads to inconsistent estimates in generalized linear models, while several other methods are either computationally intensive or lead to parameter estimates that are biased or more variable compared to the proposed multiple imputation estimator. In an extensive simulation study, we assess the bias and variability of different approaches within the context of a logistic regression model and compare variance estimation methods for the proposed multiple imputation estimator. Lastly, we apply several methods to analyze the data set from a recently-conducted GenIMS study.},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\AYWQIKBE\bernhardt15statbiosci - Statistical Methods for Generalized Linear Models with Covariates Subject to Detection Limits.pdf}
}

@inproceedings{bolognese1994,
  title = {Estimating ''non-Detects'' for Inclusion in Linear Regression},
  booktitle = {{{AMERICAN STATISTICAL ASSOCIATION}} 1994 {{PROCEEDINGS OF THE BIOPHARMACEUTICAL SECTION}}},
  author = {Bolognese, Ja and Nguyen, Hh},
  year = {1994},
  pages = {414--419},
  publisher = {{Amer Statistical Assoc}},
  address = {{Alexandria}},
  urldate = {2023-10-31},
  isbn = {978-1-883276-05-8},
  langid = {english},
  keywords = {LINEAR REGRESSION,NON-DETECTS,TOLERANCE LIMIT,WITHDRAWAL PERIOD},
  annotation = {Web of Science ID: WOS:A1994BE02M00075}
}

@article{bonner2021,
  title = {Effects of Accounting for Interval-Censored Antibody Titer Decay on Seroincidence in a Longitudinal Cohort Study of Leptospirosis},
  author = {Bonner, Katharine A. Owers and Cruz, Jaqueline S. and Sacramento, Gielson A. and {de Oliveira}, Daiana and Nery, Nivison and Carvalho, Mayara and Costa, Federico and Childs, James E. and Ko, I, Albert and Diggle, Peter J.},
  year = {2021},
  month = may,
  journal = {AMERICAN JOURNAL OF EPIDEMIOLOGY},
  volume = {190},
  number = {5},
  pages = {893--899},
  issn = {0002-9262},
  doi = {10.1093/aje/kwaa253},
  eissn = {1476-6256},
  orcid-numbers = {Ko, Albert Icksang/0000-0001-9023-2339 Costa, Federico/0000-0001-6951-2336 Diggle, Peter/0000-0003-3521-5020},
  researcherid-numbers = {Ko, Albert Icksang/P-2343-2015 Costa, Federico/G-1838-2015 Diggle, Peter J/A-3025-2009},
  unique-id = {WOS:000667749900023}
}

@article{bou-hamad2011,
  title = {A Review of Survival Trees},
  author = {{Bou-Hamad}, Imad and Larocque, Denis and {Ben-Ameur}, Hatem},
  year = {2011},
  journal = {Statist. Surv.},
  volume = {5},
  pages = {44--71},
  publisher = {{Amer Statistical Assoc}},
  address = {{Alexandria}},
  issn = {1935-7516},
  doi = {10.1214/09-SS047},
  urldate = {2023-10-30},
  abstract = {This paper presents a non-technical account of the developments in tree-based methods for the analysis of survival data with censoring. This review describes the initial developments, which mainly extended the existing basic tree methodologies to censored data as well as to more recent work. We also cover more complex models, more specialized methods, and more specific problems such as multivariate data, the use of time-varying covariates, discrete-scale survival data, and ensemble methods applied to survival trees. A data example is used to illustrate some methods that are implemented in R.},
  langid = {english},
  keywords = {bagging,BIOSTATISTICS,CART,COMPUTER-PROGRAM,discrete-time,ensemble methods,LOGICAL ANALYSIS,PROGNOSTIC CLASSIFICATION,RANDOM FORESTS,RECURSIVE PARTITION,REGRESSION TREES,right-censored data,SELECTION,SPLIT,STRATIFICATION,survival forest,Survival trees,time-varying covariate,time-varying effect},
  annotation = {Web of Science ID: WOS:000439063200003},
  file = {C:\Users\Zane\Zotero\storage\NNRZELL4\Bou-Hamad et al. - 2011 - A review of survival trees.pdf}
}

@article{boukeloua2016,
  title = {Asymptotic Normality of Kernel Estimators Based upon Incomplete Data},
  author = {Boukeloua, M. and Messaci, F.},
  year = {2016},
  journal = {J. Nonparametr. Stat.},
  volume = {28},
  number = {3},
  pages = {469--486},
  publisher = {{Taylor \& Francis Ltd}},
  address = {{Abingdon}},
  issn = {1048-5252, 1029-0311},
  doi = {10.1080/10485252.2016.1164312},
  urldate = {2023-10-30},
  abstract = {In this paper, we are concerned with nonparametric estimation of the density and the failure rate functions of a random variable X which is at risk of being censored. First, we establish the asymptotic normality of a kernel density estimator in a general censoring setup. Then, we apply our result in order to derive the asymptotic normality of both the density and the failure rate estimators in the cases of right, twice and doubly censored data. Finally, the performance and the asymptotic Gaussian behaviour of the studied estimators, based on either doubly or twice censored data, are illustrated through a simulation study.},
  langid = {english},
  keywords = {asymptotic normality,censored data,CONVERGENCE,density,DENSITY-FUNCTION,DOUBLY CENSORED-DATA,failure rate,STRONG CONSISTENCY,SURVIVAL FUNCTION},
  annotation = {Web of Science ID: WOS:000385938200002}
}

@book{breen1996,
  title = {Regression {{Models}}: {{Censored}}, {{Sample Selected}}, or {{Truncated Data}}},
  author = {Breen, Richard},
  year = {1996},
  publisher = {{SAGE Publications, Inc.}},
  doi = {10.4135/9781412985611},
  urldate = {2023-09-18},
  abstract = {{$<$}p{$>$}What techniques can social scientists use when an outcome variable for a sample is not representative of the population for whom they would like to generaliz},
  isbn = {978-1-4129-8561-1},
  langid = {english},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\DZMGW49P\\Breen - 1996 - Regression Models Censored, Sample Selected, or T.pdf;C\:\\Users\\Zane\\Zotero\\storage\\PS8375B5\\regression-models.html}
}

@article{campbell2019,
  title = {Comparison of a Time-Varying Covariate Model and a Joint Model of Time-to-Event Outcomes in the Presence of Measurement Error and Interval Censoring: Application to Kidney Transplantation},
  author = {Campbell, Kristen R. and {Juarez-Colunga}, Elizabeth and Grunwald, Gary K. and Cooper, James and Davis, Scott and Gralla, Jane},
  year = {2019},
  month = jun,
  journal = {BMC MEDICAL RESEARCH METHODOLOGY},
  volume = {19},
  number = {130},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0773-1},
  orcid-numbers = {Davis, Scott/0000-0001-5320-4435 Campbell, Kristen/0000-0002-3675-2533 Juarez-Colunga, Elizabeth/0000-0002-6369-2353},
  researcherid-numbers = {Davis, Scott/T-5322-2019 Campbell, Kristen/S-5183-2017},
  unique-id = {WOS:000473017900001}
}

@article{cauchemez2012,
  title = {Influenza {{Infection Rates}}, {{Measurement Errors}} and the {{Interpretation}} of {{Paired Serology}}},
  author = {Cauchemez, Simon and Horby, Peter and Fox, Annette and Mai, Le Quynh and Thanh, Le Thi and Thai, Pham Quang and Hoa, Le Nguyen Minh and Hien, Nguyen Tran and Ferguson, Neil M.},
  year = {2012},
  month = dec,
  journal = {PLOS Pathogens},
  volume = {8},
  number = {12},
  pages = {e1003061},
  publisher = {{Public Library of Science}},
  issn = {1553-7374},
  doi = {10.1371/journal.ppat.1003061},
  urldate = {2023-10-31},
  abstract = {Serological studies are the gold standard method to estimate influenza infection attack rates (ARs) in human populations. In a common protocol, blood samples are collected before and after the epidemic in a cohort of individuals; and a rise in haemagglutination-inhibition (HI) antibody titers during the epidemic is considered as a marker of infection. Because of inherent measurement errors, a 2-fold rise is usually considered as insufficient evidence for infection and seroconversion is therefore typically defined as a 4-fold rise or more. Here, we revisit this widely accepted 70-year old criterion. We develop a Markov chain Monte Carlo data augmentation model to quantify measurement errors and reconstruct the distribution of latent true serological status in a Vietnamese 3-year serological cohort, in which replicate measurements were available. We estimate that the 1-sided probability of a 2-fold error is 9.3\% (95\% Credible Interval, CI: 3.3\%, 17.6\%) when antibody titer is below 10 but is 20.2\% (95\% CI: 15.9\%, 24.0\%) otherwise. After correction for measurement errors, we find that the proportion of individuals with 2-fold rises in antibody titers was too large to be explained by measurement errors alone. Estimates of ARs vary greatly depending on whether those individuals are included in the definition of the infected population. A simulation study shows that our method is unbiased. The 4-fold rise case definition is relevant when aiming at a specific diagnostic for individual cases, but the justification is less obvious when the objective is to estimate ARs. In particular, it may lead to large underestimates of ARs. Determining which biological phenomenon contributes most to 2-fold rises in antibody titers is essential to assess bias with the traditional case definition and offer improved estimates of influenza ARs.},
  langid = {english},
  keywords = {Age groups,Antibodies,{Antibodies, Viral},antibody titers,Antiviral Agents,association,Biomarkers,Chloroquine,Clinical Trials as Topic,Computer Simulation,Fold increase,geometric mean concentration,H1N1,Humans,Influenza,{Influenza A Virus, H1N1 Subtype},Influenza viruses,{Influenza, Human},Interval censored,left-censored data,Linear mixed-effects model,Linear Models,maximum likelihood inference,{Models, Statistical},Monte Carlo Method,Odds Ratio,Predictive Value of Tests,Risk ratio,Seasons,Serologic Tests,Serology,Software,Spring,Treatment Outcome},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\JAUPL2TZ\\Tran et al. - 2021 - Measuring association among censored antibody tite.pdf;C\:\\Users\\Zane\\Zotero\\storage\\MXDD2WRE\\Moulton and Halsey - 1995 - A Mixture Model with Detection Limits for Regressi.pdf;C\:\\Users\\Zane\\Zotero\\storage\\PWZTHKKC\\Zhegnoun et al. - 2008 - Dealing with the non-detected and non-quantified d.pdf;C\:\\Users\\Zane\\Zotero\\storage\\NK22TNSA\\dealing-with-the-non-detected-and-non-quantified-data-the-example-of-the-serum-dioxin-data-in-t.html}
}

@article{cavalcante2018,
  title = {Mixture Models Applied to Heterogeneous Populations},
  author = {Cavalcante, Carolina V. and Goncalves, Kelly C. M.},
  year = {2018},
  month = may,
  journal = {BRAZILIAN JOURNAL OF PROBABILITY AND STATISTICS},
  volume = {32},
  number = {2},
  pages = {320--345},
  issn = {0103-0752},
  doi = {10.1214/16-BJPS345},
  orcid-numbers = {Goncalves, Kelly/0000-0002-4524-547X},
  researcherid-numbers = {Goncalves, Kelly/AAQ-1414-2020},
  unique-id = {WOS:000430260100006}
}

@article{chen2009,
  title = {A Review on Empirical Likelihood Methods for Regression},
  author = {Chen, Song Xi and Van Keilegom, Ingrid},
  year = {2009},
  month = nov,
  journal = {Test},
  volume = {18},
  number = {3},
  pages = {415--447},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/s11749-009-0159-5},
  urldate = {2023-10-30},
  abstract = {We provide a review on the empirical likelihood method for regression-type inference problems. The regression models considered in this review include parametric, semiparametric, and nonparametric models. Both missing data and censored data are accommodated.},
  langid = {english},
  keywords = {Censored data,Empirical likelihood,ESTIMATING EQUATIONS,ESTIMATOR,INFERENCE,INTERVALS,Missing data,MISSING RESPONSE DATA,MOMENT RESTRICTIONS,Nonparametric regression,Parametric regression,PARTIALLY LINEAR-MODELS,PROBABILITIES,RATIO CONFIDENCE-REGIONS,RIGHT-CENSORED-DATA,Semiparametric regression,Wilks' theorem},
  annotation = {Web of Science ID: WOS:000272178600001}
}

@article{chen2013,
  title = {A {{Bayesian}} Multiple Imputation Method for Handling Longitudinal Pesticide Data with Values below the Limit of Detection},
  author = {Chen, Haiying and Quandt, Sara A. and Grzywacz, Joseph G. and Arcury, Thomas A.},
  year = {2013},
  journal = {Environmetrics},
  volume = {24},
  number = {2},
  pages = {132--142},
  issn = {1099-095X},
  doi = {10.1002/env.2193},
  urldate = {2023-06-02},
  abstract = {Environmental and biomedical research often produces data below the limit of detection (LOD) or left-censored data. Imputing explicit values for values {$<$} LOD in a multivariate setting, such as with longitudinal data, is difficult using a likelihood-based approach. A Bayesian multiple imputation method is introduced to handle left-censored multivariate data. A Gibbs sampler, which uses an iterative process, is employed to simulate the target multivariate distribution within a Bayesian framework. Following convergence, multiple plausible data sets are generated for analysis by standard statistical methods outside of a Bayesian framework. With explicit imputed values, available variables can be analyzed as outcomes or predictors. We illustrate a practical application using longitudinal data from the Community Participatory Approach to Measuring Farmworker Pesticide Exposure (PACE3) study to evaluate the association between urinary acephate concentrations (indicating pesticide exposure) and self-reported potential pesticide poisoning symptoms. Additionally, a simulation study is conducted to evaluate the sampling property of the estimators for distributional parameters as well as regression coefficients estimated with the generalized estimating equation approach. Results demonstrated that the Bayesian multiple imputation estimates performed well in most settings, and we recommend the use of this valid and feasible approach to analyze multivariate data with values {$<$} LOD. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian,Gibbs sampler,left-censoring,limit of detection,longitudinal data,multiple imputation,multivariate,non-detections},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\E8QB7RZ8\\Chen et al. - 2013 - A Bayesian multiple imputation method for handling.pdf;C\:\\Users\\Zane\\Zotero\\storage\\CGXCX32Y\\env.html}
}

@article{cheung2019,
  title = {Regression Analysis of Fold-Increase Endpoints Using a Distributional Approach for Paired Interval-Censored Antibody Data},
  author = {Cheung, Yin Bun and Ma, Xiangmei and Lam, K. F.},
  year = {2019},
  month = jul,
  journal = {STATISTICS IN BIOPHARMACEUTICAL RESEARCH},
  volume = {11},
  number = {3},
  pages = {193--199},
  issn = {1946-6315},
  doi = {10.1080/19466315.2018.1473794},
  unique-id = {WOS:000482261300001}
}

@article{cheuvart2013,
  title = {Persistence Clinical Studies},
  author = {Cheuvart, Brigitte and Bianco, Veronique and Caubet, Magalie and Douha, Martine and Fissette, Laurence and Francois, Nancy and Sumbul, Anne},
  year = {2013},
  month = jun,
  journal = {HUMAN VACCINES \& IMMUNOTHERAPEUTICS},
  volume = {9},
  number = {6},
  pages = {1351--1357},
  issn = {2164-5515},
  doi = {10.4161/hv.24168},
  eissn = {2164-554X},
  unique-id = {WOS:000330380700033}
}

@article{christofaro2014,
  title = {{Treatment of Censored Data in Environmental Studies}},
  author = {Christofaro, Cristiano and Leao, Monica M. D.},
  year = {2014},
  journal = {Quim. Nova},
  volume = {37},
  number = {1},
  pages = {104-U142},
  publisher = {{Soc Brasileira Quimica}},
  address = {{Sao Paulo}},
  issn = {0100-4042, 1678-7064},
  doi = {10.1590/S0100-40422014000100019},
  urldate = {2023-10-30},
  abstract = {Due to the inherent limitations of the analytical methods of measurement, environmental exposure data often present observations described as below a certain detection limit, also called left-censored data. Censored data directly interferes in almost all types of statistical analyzes, including descriptive parameters, hypothesis testing, confidence intervals, correlations and regressions. In this work, we investigated the performance of the main classes of methods from major publications available in the literature, considering their advantages and limitations. Some criteria for selecting the best method of dealing with censored data are presented.},
  langid = {portuguese},
  keywords = {censored data,DATA SETS,Kaplan-Meier,limit of detection,MULTIPLE DETECTION LIMITS,QUANTIFICATION,QUANTILES,S-LANGUAGE SOFTWARE,STATISTICAL-ANALYSIS,UNCERTAINTY,VALUES,VARIABILITY,WATER-QUALITY DATA},
  annotation = {Web of Science ID: WOS:000332035600019},
  file = {C:\Users\Zane\Zotero\storage\ZVRD3WMH\Christofaro and Leao - 2014 - Treatment of Censored Data in Environmental Studie.pdf}
}

@article{cohen1963,
  title = {Progressively {{Censored Samples}} in {{Life Testing}}},
  author = {Cohen, A. Clifford},
  year = {1963},
  journal = {Technometrics},
  volume = {5},
  number = {3},
  eprint = {1266337},
  eprinttype = {jstor},
  pages = {327--339},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1266337},
  urldate = {2023-10-31},
  abstract = {In life and dosage-response studies, progressively censored samples arise when at various stages of an experiment, some though not all of the surviving sample specimens are eliminated from further observation. The sample specimens remaining after each stage of censoring are continued under observation until failure or until a subsequent stage of censoring. In this paper maximum likelihood estimators of the distribution parameters are derived for the normal, and for the exponential distribution when samples are progressively censored.},
  file = {C:\Users\Zane\Zotero\storage\XBCQFQZZ\Cohen - 1963 - Progressively Censored Samples in Life Testing.pdf}
}

@book{cohen1991,
  title = {Truncated and {{Censored Samples}}: Theory and Applications},
  author = {Cohen, A. Clifford},
  year = {1991},
  series = {Statistics: {{Textbooks}} and {{Monographs}}},
  number = {119},
  publisher = {{CRC Press}},
  abstract = {This book deals with the development of methodology for the analysis of truncated and censored sample data. It is primarily intended as a handbook for practitioners who need simple and efficient methods for the analysis of incomplete sample data.},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\WAN4QECC\(Statistics textbooks and monographs 119) Cohen, A. Clifford - Truncated and Censored Samples_ Theory and Applications-Marcel Dekker (1991).pdf}
}

@article{coudeville2010,
  title = {Relationship between Haemagglutination-Inhibiting Antibody Titres and Clinical Protection against Influenza: Development and Application of a Bayesian Random-Effects Model},
  author = {Coudeville, Laurent and Bailleux, Fabrice and Riche, Benjamin and Megas, Francoise and Andre, Philippe and Ecochard, Rene},
  year = {2010},
  month = mar,
  journal = {BMC MEDICAL RESEARCH METHODOLOGY},
  volume = {10},
  number = {18},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-10-18},
  orcid-numbers = {Naselsky, Pavel D/0000-0002-8891-0273},
  researcherid-numbers = {Naselsky, Pavel D/M-4799-2014},
  unique-id = {WOS:000276568800001}
}

@article{cramer2023,
  title = {Ordered and Censored Lifetime Data in Reliability: {{An}} Illustrative Review},
  shorttitle = {Ordered and Censored Lifetime Data in Reliability},
  author = {Cramer, Erhard},
  year = {2023},
  month = mar,
  journal = {Wiley Interdiscip. Rev.-Comput. Stat.},
  volume = {15},
  number = {2},
  pages = {e1571},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {1939-0068},
  doi = {10.1002/wics.1571},
  urldate = {2023-10-30},
  abstract = {The present review provides a survey on basic models of ordered data and censoring mechanisms with a focus on lifetime data, failure data, and reliability applications. Throughout, illustrations of the data generation process as well as of the censoring mechanisms are used to visualize these procedures. By example we present basic results assuming a life testing model with independent and identically distributed measurements and focus on selected inferential results for exponentially distributed lifetimes. In particular, we aim to illustrate similarities between the models as well as to highlight some interesting exact statistical results. It is not intended to survey all possible model assumptions, probabilistic results, and used inferential methods used in this framework. For this purpose as well as for further reading, we provide an extensive bibliography. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Reliability, Survivability, and Quality Control Algorithms and Computational Methods {$>$} Maximum Likelihood Methods Data: Types and Structure {$>$} Traditional Statistical Data},
  langid = {english},
  keywords = {censoring mechanisms,EXACT LIKELIHOOD INFERENCE,EXACT NONPARAMETRIC CONFIDENCE,K EXPONENTIAL POPULATIONS,lifetime data,MARGINAL DISTRIBUTIONS,MAXIMUM-LIKELIHOOD,MINIMAL-REPAIR,order statistics,ordered data,RECORD VALUES,reliability,SAMPLES,STATISTICAL-ANALYSIS,TOLERANCE INTERVALS},
  annotation = {Web of Science ID: WOS:000700824900001}
}

@article{dagne2013,
  title = {Bayesian Inference for Skew-Normal Mixture Models with Left-Censoring},
  author = {Dagne, Getachew A.},
  year = {2013},
  month = sep,
  journal = {JOURNAL OF BIOPHARMACEUTICAL STATISTICS},
  volume = {23},
  number = {5},
  pages = {1023--1041},
  issn = {1054-3406},
  doi = {10.1080/10543406.2013.813517},
  eissn = {1520-5711},
  unique-id = {WOS:000323409500006}
}

@article{darrigo2021,
  title = {Methods to {{Analyse Time-to-Event Data}}: {{The Kaplan-Meier Survival Curve}}},
  shorttitle = {Methods to {{Analyse Time-to-Event Data}}},
  author = {D'Arrigo, Graziella and Leonardis, Daniela and Abd ElHafeez, Samar and Fusaro, Maria and Tripepi, Giovanni and Roumeliotis, Stefanos},
  year = {2021},
  month = sep,
  journal = {Oxidative Med. Cell. Longev.},
  volume = {2021},
  pages = {2290120},
  publisher = {{Hindawi Ltd}},
  address = {{London}},
  issn = {1942-0900, 1942-0994},
  doi = {10.1155/2021/2290120},
  urldate = {2023-10-30},
  abstract = {Studies performed in the field of oxidative medicine and cellular longevity frequently focus on the association between biomarkers of cellular and molecular mechanisms of oxidative stress as well as of aging, immune function, and vascular biology with specific time to event data, such as mortality and organ failure. Indeed, time-to-event analysis is one of the most important methodologies used in clinical and epidemiological research to address etiological and prognostic hypotheses. Survival data require adequate methods of analyses. Among these, the Kaplan-Meier analysis is the most used one in both observational and interventional studies. In this paper, we describe the mathematical background of this technique and the concept of censoring (right censoring, interval censoring, and left censoring) and report some examples demonstrating how to construct a Kaplan-Meier survival curve and how to apply this method to provide an answer to specific research questions.},
  langid = {english},
  annotation = {Web of Science ID: WOS:000703531200004},
  file = {C:\Users\Zane\Zotero\storage\RSR555CV\D'Arrigo et al. - 2021 - Methods to Analyse Time-to-Event Data The Kaplan-.pdf}
}

@article{delabre2017,
  title = {Immunity against Influenza {{A}}({{H1N1}}) Infections Is Determined by Age at the Time of Initial Strain Circulation},
  author = {Delabre, R. M. and Salez, N. and Lapidus, N. and Lemaitre, M. and {Leruez-Ville}, M. and {de Lamballerie}, X. and Carrat, F.},
  year = {2017},
  month = jan,
  journal = {EPIDEMIOLOGY AND INFECTION},
  volume = {145},
  number = {1},
  pages = {141--147},
  issn = {0950-2688},
  doi = {10.1017/S0950268816002156},
  eissn = {1469-4409},
  orcid-numbers = {Carrat, Fabrice/0000-0002-8672-7918 Lapidus, N./0000-0002-4837-1068},
  researcherid-numbers = {Carrat, Fabrice/AAU-8480-2020 Lapidus, N./AAT-4479-2020},
  unique-id = {WOS:000390101500016}
}

@article{desousa2018,
  title = {On a Tobit-{{Birnbaum-Saunders}} Model with an Application to Medical Data},
  author = {Desousa, Mario F. and Saulo, Helton and Leiva, Victor and Scalco, Paulo},
  year = {2018},
  journal = {JOURNAL OF APPLIED STATISTICS},
  volume = {45},
  number = {5},
  pages = {932--955},
  issn = {0266-4763},
  doi = {10.1080/02664763.2017.1322559},
  eissn = {1360-0532},
  orcid-numbers = {Saulo, Helton/0000-0002-4467-8652 Scalco, Paulo/0000-0002-0667-0422 Leiva, Victor/0000-0003-4755-3270 Saulo, Helton/0000-0002-4467-8652 de Sousa, Mario Fernando/0000-0001-6792-9162},
  researcherid-numbers = {Saulo, Helton/S-1710-2019 Scalco, Paulo/AAK-4448-2021 Leiva, Victor/AAM-7834-2021 Saulo, Helton/R-4670-2016},
  unique-id = {WOS:000426925100010}
}

@article{donovan2019,
  title = {Nonparametric Inference for Immune Response Thresholds of Risk in Vaccine Studies},
  author = {Donovan, Kevin M. and Hudgens, Michael G. and Gilbert, Peter B.},
  year = {2019},
  month = jun,
  journal = {ANNALS OF APPLIED STATISTICS},
  volume = {13},
  number = {2},
  pages = {1147--1165},
  issn = {1932-6157},
  doi = {10.1214/18-AOAS1237},
  unique-id = {WOS:000471840900018}
}

@article{dowd2009,
  title = {Socioeconomic Differentials in Immune Response},
  author = {Dowd, Jennifer Beam and Aiello, Allison E.},
  year = {2009},
  month = nov,
  journal = {EPIDEMIOLOGY},
  volume = {20},
  number = {6},
  pages = {902--908},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181bb5302},
  eissn = {1531-5487},
  orcid-numbers = {Dowd, Jennifer B/0000-0003-2006-5656},
  researcherid-numbers = {Dowd, Jennifer B/C-1127-2012},
  unique-id = {WOS:000270874000020}
}

@article{eilers2012,
  title = {Quantile Regression for the Statistical Analysis of Immunological Data with Many Non-Detects},
  author = {Eilers, Paul H. C. and Roder, Esther and Savelkoul, Huub F. J. and {van Wijk}, Roy Gerth},
  year = {2012},
  month = jul,
  journal = {BMC Immunol.},
  volume = {13},
  pages = {37},
  publisher = {{BMC}},
  address = {{London}},
  issn = {1471-2172},
  doi = {10.1186/1471-2172-13-37},
  urldate = {2023-10-31},
  abstract = {Background: Immunological parameters are hard to measure. A well-known problem is the occurrence of values below the detection limit, the non-detects. Non-detects are a nuisance, because classical statistical analyses, like ANOVA and regression, cannot be applied. The more advanced statistical techniques currently available for the analysis of datasets with non-detects can only be used if a small percentage of the data are non-detects. Methods and results: Quantile regression, a generalization of percentiles to regression models, models the median or higher percentiles and tolerates very high numbers of non-detects. We present a non-technical introduction and illustrate it with an implementation to real data from a clinical trial. We show that by using quantile regression, groups can be compared and that meaningful linear trends can be computed, even if more than half of the data consists of non-detects. Conclusion: Quantile regression is a valuable addition to the statistical methods that can be used for the analysis of immunological datasets with non-detects.},
  langid = {english},
  keywords = {Data analysis,Immunological data,Non-detects,Outliers,Quantile regression,Robustness,Soluble biological markers,Statistical},
  annotation = {Web of Science ID: WOS:000309002400002},
  file = {C:\Users\Zane\Zotero\storage\W87VGBQU\Eilers et al. - 2012 - Quantile regression for the statistical analysis o.pdf}
}

@article{el-shaarawi1992,
  title = {Replacement of Censored Observations by a Constant: {{An}} Evaluation},
  shorttitle = {Replacement of Censored Observations by a Constant},
  author = {{El-Shaarawi}, A. H. and Esterby, S. R.},
  year = {1992},
  month = jun,
  journal = {Water Research},
  volume = {26},
  number = {6},
  pages = {835--844},
  issn = {0043-1354},
  doi = {10.1016/0043-1354(92)90015-V},
  urldate = {2023-10-31},
  abstract = {In the analysis of water quality data, samples with concentrations below the analytical detection limit are sometimes replaced by a constant between 0 and the detection limit. The performance of this replacement method has previously been examined in Monte Carlo studies by several authors. In this paper, expressions for the expectation of the sample mean and variance, obtained under the replacement method, are given for normally and lognormally distributed data. These expressions provide a simple means of calculating the bias and mean square error of the estimators used in the replacement method and thus large numbers of samples do not need to be generated by computer to evaluate the method. Thus, in a particular situation, the bias to be expected can be calculated if limits on the ranges of the mean, variance and proportion censored are available. It is shown that without knowledge of these qualities, which is the usual situation when data are being analyzed, the size and direction of bias is unknown. More objective methods of estimation are the methods of maximum likelihood and log-probability regression which are either described or made available through references. Two examples are analyzed by the latter methods and the results compared with those of the replacement method.},
  keywords = {bias,detection limit,likelihood function,log probability regression,mean,normal and lognormal distributions,variance,water quality},
  file = {C:\Users\Zane\Zotero\storage\JAWE3WEZ\El-Shaarawi and Esterby - 1992 - Replacement of censored observations by a constant.pdf}
}

@article{emura2022,
  title = {Left-Truncated and Right-Censored Field Failure Data: {{Review}} of Parametric Analysis for Reliability},
  shorttitle = {Left-Truncated and Right-Censored Field Failure Data},
  author = {Emura, Takeshi and Michimae, Hirofumi},
  year = {2022},
  month = nov,
  journal = {Qual. Reliab. Eng. Int.},
  volume = {38},
  number = {7},
  pages = {3919--3934},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0748-8017, 1099-1638},
  doi = {10.1002/qre.3161},
  urldate = {2023-10-30},
  abstract = {In field reliability analyses, a data collection period is given to monitor the failure events from the field. Left-truncation arises due to early failures occurring before the data collection period, and right-censoring arises for late failures occurring beyond the monitoring period. Naive analyses of left-truncated and right-censored data lead to biased estimation of the population lifetime of interest. A variety of models and methods have been developed to analyze the left-truncated and right-censored data for field reliability analyses. The goal of the paper is to review the existing models and methods for fitting left-truncated and right-censored data. Our review includes the existing statistical models, such as the exponential, Weibull, lognormal, gamma, Gompertz, Lomax, and spline models. We comprehensively review the statistical issues of maximum likelihood estimation, model selection, residual lifetime prediction, and Bayesian methods. Some of these methods are illustrated through the field reliability analysis of the electric power transformer dataset.},
  langid = {english},
  keywords = {Akaike's information criterion,COMPETING RISKS,EM algorithm,LIKELIHOOD INFERENCE,lognormal distribution,MODEL SELECTION,Newton-Raphson algorithm,NONPARAMETRIC-ESTIMATION,Reliability,STATISTICAL-METHODS,WEIBULL,Weibull distribution},
  annotation = {Web of Science ID: WOS:000818933600001}
}

@article{fievet2010,
  title = {Dealing with Non-Detect Values in Time-Series Measurements of Radionuclide Concentration in the Marine Environment},
  author = {Fievet, Bruno and Della Vedova, Claire},
  year = {2010},
  month = jan,
  journal = {J. Environ. Radioact.},
  volume = {101},
  number = {1},
  pages = {1--7},
  publisher = {{Elsevier Sci Ltd}},
  address = {{Oxford}},
  issn = {0265-931X, 1879-1700},
  doi = {10.1016/j.jenvrad.2009.07.007},
  urldate = {2023-10-30},
  abstract = {The attention of scientists in the field of environmental radioactivity is drawn to statistical methods recommended by Dennis Helsel for dealing with datasets including measurements that fall below the detection limits, as often encountered in environmental monitoring programmes. The methods are described by Helsel in his book entitled "Nondetects and Data Analysis: Statistics for Censored Environmental Data" (John Wiley and Sons, New York, 2005, 250p). These methods are applied to a data subset (using data from France) of the Radioactive Substance Committee (OSPAR commission for the protection of the marine environment of the North-East Atlantic), corresponding to time-series measurements of Cs-137 concentration in seaweed in the vicinity of the Areva NC reprocessing plant at La Hague, which is used as an illustrative example. Despite the presence of 163 non-detect values out of 514 measurements, it is possible to estimate descriptive parameters and perform statistical tests to compare concentration levels between two periods of time. Finally, evidence is obtained for an overall decreasing trend with time. The benefits of these statistical methods for data analysis are discussed. (C) 2009 Elsevier Ltd. All rights reserved.},
  langid = {english},
  keywords = {Detection limit,Environment monitoring,MULTIPLE DETECTION LIMITS,Non-detect values,OSPAR commission,S-LANGUAGE SOFTWARE,Statistical analysis,STATISTICAL-ANALYSIS,WATER-QUALITY DATA},
  annotation = {Web of Science ID: WOS:000273150600001}
}

@article{gelman2004,
  title = {Bayesian {{Analysis}} of {{Serial Dilution Assays}}},
  author = {Gelman, Andrew and Chew, Ginger L. and Shnaidman, Michael},
  year = {2004},
  month = jun,
  journal = {Biometrics},
  volume = {60},
  number = {2},
  pages = {407--417},
  issn = {0006341X},
  doi = {10.1111/j.0006-341X.2004.00185.x},
  urldate = {2023-09-25},
  abstract = {In a serial dilution assay, the concentration of a compound is estimated by combining measurements of several different dilutions of an unknown sample. The relation between concentration and measurement is nonlinear and heteroscedastic, and so it is not appropriate to weight these measurements equally. In the standard existing approach for analysis of these data, a large proportion of the measurements are discarded as being above or below detection limits. We present a Bayesian method for jointly estimating the calibration curve and the unknown concentrations using all the data. Compared to the existing method, our estimates have much lower standard errors and give estimates even when all the measurements are outside the ``detection limits.'' We evaluate our method empirically using laboratory data on cockroach allergens measured in house dust samples. Our estimates are much more accurate than those obtained using the usual approach. In addition, we develop a method for determining the ``effective weight'' attached to each measurement, based on a local linearization of the estimated model. The effective weight can give insight into the information conveyed by each data point and suggests potential improvements in design of serial dilution experiments.},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\HCXTALJN\Biometrics - 2004 - Gelman.pdf}
}

@article{gilbert2019,
  title = {Assessing Pharmacokinetic Marker Correlates of Outcome, with Application to Antibody Prevention Efficacy Trials},
  author = {Gilbert, Peter B. and Zhang, Yuanyuan and Rudnicki, Erika and Huang, Yunda},
  year = {2019},
  month = oct,
  journal = {STATISTICS IN MEDICINE},
  volume = {38},
  number = {23},
  pages = {4503--4518},
  issn = {0277-6715},
  doi = {10.1002/sim.8310},
  earlyaccessdate = {JUL 2019},
  eissn = {1097-0258},
  unique-id = {WOS:000476330800001}
}

@article{gilbert2023,
  title = {A Controlled Effects Approach to Assessing Immune Correlates of Protection},
  author = {Gilbert, Peter B. and Fong, Youyi and Kenny, Avi and Carone, Marco},
  year = {2023},
  month = oct,
  journal = {BIOSTATISTICS},
  volume = {24},
  number = {4},
  pages = {850--865},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxac024},
  earlyaccessdate = {JUL 2022},
  eissn = {1468-4357},
  unique-id = {WOS:000826587400001}
}

@article{goeyvaerts2015,
  title = {Multi-Disease Analysis of Maternal Antibody Decay Using Non-Linear Mixed Models Accounting for Censoring},
  author = {Goeyvaerts, Nele and Leuridan, Elke and Faes, Christel and Van Damme, Pierre and Hens, Niel},
  year = {2015},
  month = sep,
  journal = {STATISTICS IN MEDICINE},
  volume = {34},
  number = {20},
  pages = {2858--2871},
  issn = {0277-6715},
  doi = {10.1002/sim.6518},
  eissn = {1097-0258},
  orcid-numbers = {Faes, Christel/0000-0002-1878-9869 Hens, Niel/0000-0003-1881-0637 Leuridan, Elke/0000-0001-7482-799X van damme, pierre/0000-0002-8642-1249 Goeyvaerts, Nele/0000-0003-0526-5318},
  researcherid-numbers = {Faes, Christel/E-1384-2017 Hens, Niel/M-4445-2017 van damme, pierre/I-4846-2013},
  unique-id = {WOS:000358421500005}
}

@article{goldkuhle2023,
  title = {Meta-Epidemiological Review Identified Variable Reporting and Handling of Time-to-Event Analyses in Publications of Trials Included in Meta-Analyses of Systematic Reviews},
  author = {Goldkuhle, Marius and Hirsch, Caroline and Iannizzi, Claire and Bora, Ana-Mihaela and Bender, Ralf and {van Dalen}, Elvira C. and Hemkens, Lars G. and Trivella, Marialene and Monsef, Ina and Kreuzberger, Nina and Skoetz, Nicole},
  year = {2023},
  month = jul,
  journal = {J. Clin. Epidemiol.},
  volume = {159},
  pages = {174--189},
  publisher = {{Elsevier Science Inc}},
  address = {{New York}},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2023.05.023},
  urldate = {2023-10-30},
  abstract = {Objectives: Previous findings indicate limited reporting of systematic reviews with meta-analyses of time-to-event (TTE) outcomes. We assessed corresponding available information in trial publications included in such meta-analyses.Study Design and Setting: We extracted data from all randomized trials in pairwise, hazard ratio (HR)-based meta-analyses of pri-mary outcomes and overall survival of 50 systematic reviews systematically identified from the Cochrane Database and Core Clinical Jour-nals. Data on methods and characteristics relevant for TTE analysis of reviews, trials, and outcomes were extracted.Results: Meta-analyses included 235 trials with 315 trial analyses. Most prominently assessed was overall survival (91\%). Definitions (61\%), censoring reasons (41\%), and follow-up specifications (56\%) for trial outcomes were often missing. Available TTE data per trial were most frequently survival curves (83\%), log-rank P values (76\%), and HRs (72\%). When trial TTE data recalculation was reported, reviews mostly specified HRs or P values (each 5\%). Reviews primarily included intention-to-treat analyses (64\%) and analyses not adjusted for covariates (25\%). Except for missing outcome data, TTE-relevant trial characteristics, for example, informative censoring, treatment switching, and proportional hazards, were sporadically addressed in trial publications. Reporting limitations in trial publications translate to the review level.Conclusion: TTE (meta)-analyses, in trial and review publications, need clear reporting standards.\& COPY; 2023 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
  langid = {english},
  keywords = {GUIDELINES,Meta-analysis,MISSING DATA,OUTCOMES,Randomized trials,Reporting quality,SURVIVAL ANALYSES,Survival analysis,Systematic review,Time-to-event outcomes},
  annotation = {Web of Science ID: WOS:001027546900001},
  file = {C:\Users\Zane\Zotero\storage\JW5HA9NR\Goldkuhle et al. - 2023 - Meta-epidemiological review identified variable re.pdf}
}

@article{gomez1992,
  title = {Estimation of Induction Distributions with Doubly Censored-Data and Application to Aids},
  author = {GOMEZ, G},
  year = {1992},
  journal = {THEORY OF PROBABILITY AND ITS APPLICATIONS},
  volume = {37},
  number = {1},
  pages = {32--39},
  issn = {0040-585X},
  unique-id = {WOS:A1992LC15300006}
}

@incollection{gomez1992a,
  title = {Survival {{Analysis For Left Censored Data}}},
  booktitle = {Survival {{Analysis}}: {{State}} of the {{Art}}},
  author = {Gomez, Guadalupe and Juli{\`a}, Olga and Utzet, Frederic and Moeschberger, Melvin L.},
  editor = {Klein, John P. and Goel, Prem K.},
  year = {1992},
  series = {Nato {{Science}}},
  pages = {269--288},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-7983-4_16},
  urldate = {2023-10-30},
  abstract = {A left censoring scheme is such that the random variable of interest, X, is only observed if it is greater than or equal to a left censoring variable L, otherwise L is observed. The analysis is then based on the pair of random variables (U, {$\delta$}) where U = max(L, X) and {$\delta$} = 1\{L {$\leq$} X\}. The problem concerns the estimation of the survival function SX(t) = Pr\{X {$>$} t\} from a left censored sample where X is assumed to be independent of L. We derive a Left-Kaplan-Meier estimator, \$\$\textbackslash hat\{\textbackslash textup\{S\}\}\_\{\textbackslash textup\{X\}\}\$\$, as a solution of a backward D\'oleans differential equation. It is proved that this Left-Kaplan-Meier estimator is self-consistent, thus a generalized maximum likelihood estimator. Following Efron's (1967) technique for the case of a right-censored scheme, it is shown that the Left-Kaplan-Meier estimator is the same estimator you would obtain through a redistribution to the left algorithm. The consistency of the Left-Kaplan-Meier estimator is established. The influence curves corresponding to \$\$\textbackslash hat\{\textbackslash textup\{S\}\}\_\{\textbackslash textup\{X\}\}\$\$, are calculated. This provides an alternative derivation of the asymptotic variance of \$\$\textbackslash hat\{\textbackslash textup\{S\}\}\_\{\textbackslash textup\{X\}\}\$\$, (Reid, 1981). The asymptotic normality then follows through standard arguments.},
  isbn = {978-94-015-7983-4},
  langid = {english},
  keywords = {Asymptotic Normality,Cumulative Hazard Function,Descent Data,Strong Consistency,Supremum Norm}
}

@article{gomez2009,
  title = {Tutorial on Methods for Interval-Censored Data and Their Implementation in {{R}}},
  author = {G{\'o}mez, Guadalupe and Calle, M Luz and Oller, Ramon and Langohr, Klaus},
  year = {2009},
  month = dec,
  journal = {Statistical Modelling},
  volume = {9},
  number = {4},
  pages = {259--297},
  publisher = {{SAGE Publications India}},
  issn = {1471-082X},
  doi = {10.1177/1471082X0900900402},
  urldate = {2023-08-15},
  abstract = {Interval censoring is encountered in many practical situations when the event of interest cannot be observed and it is only known to have occurred within a time window. The theory for the analysis of interval-censored data has been developed over the past three decades and several reviews have been written. However, it is still a common practice in medical and reliability studies to simplify the interval censoring structure of the data into a more standard right censoring situation by, for instance, imputing the midpoint of the censoring interval. The availability of software for right censoring might well be the main reason for this simplifying practice. In contrast, several methods have been developed to deal with interval-censored data and the corresponding algorithms to make the procedures feasible are scattered across the statistical software or remain behind the personal computers of many researchers. The purpose of this tutorial is to present, in a pedagogical and unified manner, the methodology and the available software for analyzing interval-censored data. The paper covers frequentist non-parametric, parametric and semiparametric estimating approaches, non-parametric tests for comparing survival curves and a section on simulation of interval-censored data. The methods and the software are described using the data from a dental study.},
  langid = {english}
}

@incollection{gomezmelis2022,
  title = {Regression {{Analysis}} with {{Interval-Censored Covariates}}. {{Application}} to {{Liquid Chromatography}}},
  booktitle = {Emerging {{Topics}} in {{Modeling Interval-Censored Survival Data}}},
  author = {G{\'o}mez Melis, Guadalupe and {Marhuenda-Mu{\~n}oz}, Mar{\'i}a and Langohr, Klaus},
  editor = {Sun, Jianguo and Chen, Ding-Geng},
  year = {2022},
  series = {{{ICSA Book Series}} in {{Statistics}}},
  pages = {271--294},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-12366-5_14},
  urldate = {2023-08-15},
  abstract = {We extend the linear regression model accounting for an interval-censored covariate of G\'omez et al. (Stat Med 22:409\textendash 425, 2003) to a generalized linear model that accommodates non-normal responses belonging to an exponential family. We redefine the original likelihood function to include exactly observed values of the same covariate. We propose two goodness-of-fit measures that accommodate interval-censored covariates. The data set that has motivated this work comes from the Metabolomic Analysis area when the interest is to assess the association of any metabolite extracted from a human sample and measured by liquid chromatography with anthropometric, clinical, and biochemical parameters and potentially any other response variable of interest. The concentration of compounds, such as plasma carotenoids, cannot always be exactly measured because of their low quantity in the body and the limitations of the methods and equipment. Up to this date, there is no consensus as to how data under the limit of quantitation or even detection should be treated.},
  isbn = {978-3-031-12366-5},
  langid = {english},
  keywords = {Detection limit,Interval censoring,Linear regression model,Quantitation Limit,Residual analysis,Spectrometry},
  file = {C:\Users\Zane\Zotero\storage\3IYKZBBX\Gómez Melis et al. - 2022 - Regression Analysis with Interval-Censored Covaria.pdf}
}

@article{groth2017,
  title = {Bivariate {{Left-Censored Bayesian Model}} for {{Predicting Exposure}}: {{Preliminary Analysis}} of {{Worker Exposure}} during the {{Deepwater Horizon Oil Spill}}},
  shorttitle = {Bivariate {{Left-Censored Bayesian Model}} for {{Predicting Exposure}}},
  author = {Groth, Caroline and Banerjee, Sudipto and Ramachandran, Gurumurthy and Stenzel, Mark R. and Sandler, Dale P. and Blair, Aaron and Engel, Lawrence S. and Kwok, Richard K. and Stewart, Patricia A.},
  year = {2017},
  month = jan,
  journal = {Annals of Work Exposures and Health},
  volume = {61},
  number = {1},
  pages = {76--86},
  issn = {2398-7308, 2398-7316},
  doi = {10.1093/annweh/wxw003},
  urldate = {2023-06-20},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\TJISVZX9\groth17aweh - Bivariate Left-Censored Bayesian Model for Predicting Exposure.pdf}
}

@article{groth2018,
  title = {Multivariate Left-Censored {{Bayesian}} Modeling for Predicting Exposure Using Multiple Chemical Predictors},
  author = {Groth, Caroline P. and Banerjee, Sudipto and Ramachandran, Gurumurthy and Stenzel, Mark R. and Stewart, Patricia A.},
  year = {2018},
  journal = {Environmetrics},
  volume = {29},
  number = {4},
  pages = {e2505},
  issn = {1099-095X},
  doi = {10.1002/env.2505},
  urldate = {2023-08-15},
  abstract = {Environmental health exposures to airborne chemicals often originate from chemical mixtures. Environmental health professionals may be interested in assessing exposure to one or more of the chemicals in these mixtures, but often, exposure measurement data are not available, either because measurements were not collected/assessed for all exposure scenarios of interest or because some of the measurements were below the analytical methods' limits of detection (i.e., censored). In some cases, based on chemical laws, two or more components may have linear relationships with one another, whether in single or multiple mixtures. Although bivariate analyses can be used if the correlation is high, correlations are often low. To serve this need, this paper develops a multivariate framework for assessing exposure using relationships of the chemicals present in these mixtures. This framework accounts for censored measurements in all chemicals, allowing us to develop unbiased exposure estimates. We assessed our model's performance against simpler models at a variety of censoring levels and assessed our model's 95\% coverage. We applied our model to assess vapor exposure from measurements of three chemicals in crude oil taken on the Ocean Intervention III during the Deepwater Horizon oil spill response and cleanup.},
  copyright = {Copyright \textcopyright{} 2018 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {chemical mixtures,correlations,Deepwater Horizon oil spill,exposure assessment},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\63ZYF39R\\Groth et al. - 2018 - Multivariate left-censored Bayesian modeling for p.pdf;C\:\\Users\\Zane\\Zotero\\storage\\R32W3RNU\\groth18environmetrics - Multivariate left‐censored Bayesian modeling for predicting exposure using multiple.pdf;C\:\\Users\\Zane\\Zotero\\storage\\FL93Z66U\\env.html}
}

@article{guo2014,
  title = {An Overview of Semiparametric Models in Survival Analysis},
  author = {Guo, Shaojun and Zeng, Donglin},
  year = {2014},
  month = aug,
  journal = {J. Stat. Plan. Infer.},
  volume = {151},
  pages = {1--16},
  publisher = {{Elsevier}},
  address = {{Amsterdam}},
  issn = {0378-3758, 1873-1171},
  doi = {10.1016/j.jspi.2013.10.008},
  urldate = {2023-10-30},
  abstract = {We provide an overview of semiparametric models commonly used in survival analysis, including proportional hazards model, proportional odds models and linear transformation models. The applications of these models to different types of censored data, either univariate or multivariate survival analysis, are given. For each case, inference procedures using censored observations are discussed. (C) 2013 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {ASYMPTOTIC THEORY,Counting process,EFFICIENT ESTIMATION,Empirical process,FAILURE TIME DATA,Frailty model,Interval censoring,JOINT ANALYSIS,MAXIMUM-LIKELIHOOD-ESTIMATION,Nonparametric maximum likelihood,ODDS REGRESSION,Partial likelihood,Proportional hazards model,PROPORTIONAL HAZARDS MODEL,Proportional odds model,RECURRENT EVENTS,REGRESSION-ANALYSIS,Right censoring,Transformation model,TRANSFORMATION MODELS},
  annotation = {Web of Science ID: WOS:000338597200001}
}

@article{hecht2018,
  title = {Utilization of Data below the Analytical Limit of Quantitation in Pharmacokinetic Analysis and Modeling: Promoting Interdisciplinary Debate},
  shorttitle = {Utilization of Data below the Analytical Limit of Quantitation in Pharmacokinetic Analysis and Modeling},
  author = {Hecht, Max and Veigure, R{\=u}ta and Couchman, Lewis and S Barker, Charlotte I and Standing, Joseph F and Takkis, Kalev and Evard, Hanno and Johnston, Atholl and Herodes, Koit and Leito, Ivo and Kipper, Karin},
  year = {2018},
  month = aug,
  journal = {Bioanalysis},
  volume = {10},
  number = {15},
  pages = {1229--1248},
  issn = {1757-6180, 1757-6199},
  doi = {10.4155/bio-2018-0078},
  urldate = {2023-05-31},
  abstract = {Traditionally, bioanalytical laboratories do not report actual concentrations for samples with results below the LOQ (BLQ) in pharmacokinetic studies. BLQ values are outside the method calibration range established during validation and no data are available to support the reliability of these values. However, ignoring BLQ data can contribute to bias and imprecision in model-based pharmacokinetic analyses. From this perspective, routine use of BLQ data would be advantageous. We would like to initiate an interdisciplinary debate on this important topic by summarizing the current concepts and use of BLQ data by regulators, pharmacometricians and bioanalysts. Through introducing the limit of detection and evaluating its variability,~BLQ data could be released and utilized appropriately for pharmacokinetic research.},
  langid = {english},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\6RGE6NWD\\hecht - BLQ paper_Final_resubmissionv2_clean_-1.pdf;C\:\\Users\\Zane\\Zotero\\storage\\D8PBMKEX\\Hecht et al. - 2018 - Utilization of data below the analytical limit of .pdf;C\:\\Users\\Zane\\Zotero\\storage\\WVKBC9K3\\hecht2018bioanalysis.pdf}
}

@article{helsel2010,
  title = {Much {{Ado About Next}} to {{Nothing}}: {{Incorporating Nondetects}} in {{Science}}},
  shorttitle = {Much {{Ado About Next}} to {{Nothing}}},
  author = {Helsel, Dennis},
  year = {2010},
  month = apr,
  journal = {The Annals of Occupational Hygiene},
  volume = {54},
  number = {3},
  pages = {257--262},
  issn = {0003-4878},
  doi = {10.1093/annhyg/mep092},
  urldate = {2023-10-11},
  abstract = {A great many papers and one textbook have been published on the topic of how to incorporate `nondetects', low-level values reported only as below a detection limit, into statistical analyses. This is of interest not only in occupational hygiene but also in environmental sciences and astronomy, among other fields. Here, the literature is reviewed from the earliest known publication on the topic \&gt;40 years ago and recommendations contrasted. I have tried to pull some unifying conclusions out of the mix, ending with four suggestions I believe all can agree on. See if you agree with me.},
  file = {C:\Users\Zane\Zotero\storage\ZEMBR59S\Helsel - 2010 - Much Ado About Next to Nothing Incorporating Nond.pdf}
}

@book{helsel2012,
  title = {Statistics for Censored Environmental Data Using {{Minitab}} and {{R}}},
  author = {Helsel, Dennis R.},
  year = {2012},
  series = {Wiley Series in Statistics in Practice},
  edition = {2nd ed},
  publisher = {{Wiley}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-470-47988-9},
  langid = {english},
  lccn = {GE45.S73 H45 2012},
  keywords = {Environmental sciences,Measurement Statistical methods,Minitab,Pollution,R (Computer program language),Statistical methods},
  annotation = {OCLC: ocn748290711},
  file = {C:\Users\Zane\Zotero\storage\652R7ANR\Helsel 2011 - Statistics for Censored Environmental Data Using Minitab and R.pdf}
}

@article{hewett2007,
  title = {A Comparison of Several Methods for Analyzing Censored Data},
  author = {Hewett, Paul and Ganser, Gary H.},
  year = {2007},
  month = oct,
  journal = {Ann Occup Hyg},
  volume = {51},
  number = {7},
  pages = {611--632},
  issn = {0003-4878},
  doi = {10.1093/annhyg/mem045},
  abstract = {The purpose of this study was to compare the performance of several methods for statistically analyzing censored datasets [i.e. datasets that contain measurements that are less than the field limit-of-detection (LOD)] when estimating the 95th percentile and the mean of right-skewed occupational exposure data. The methods examined were several variations on the maximum likelihood estimation (MLE) and log-probit regression (LPR) methods, the common substitution methods, several non-parametric (NP) quantile methods for the 95th percentile and the NP Kaplan-Meier (KM) method. Each method was challenged with computer-generated censored datasets for a variety of plausible scenarios where the following factors were allowed to vary randomly within fairly wide ranges: the true geometric standard deviation, the censoring point or LOD and the sample size. This was repeated for both a single-laboratory scenario (i.e. single LOD) and a multiple-laboratory scenario (i.e. three LODs) as well as a single lognormal distribution scenario and a contaminated lognormal distribution scenario. Each method was used to estimate the 95th percentile and mean for the censored datasets (the NP quantile methods estimated only the 95th percentile). For each scenario, the method bias and overall imprecision (as indicated by the root mean square error or rMSE) were calculated for the 95th percentile and mean. No single method was unequivocally superior across all scenarios, although nearly all of the methods excelled in one or more scenarios. Overall, only the MLE- and LPR-based methods performed well across all scenarios, with the robust versions generally showing less bias than the standard versions when challenged with a contaminated lognormal distribution and multiple LODs. All of the MLE- and LPR-based methods were remarkably robust to departures from the lognormal assumption, nearly always having lower rMSE values than the NP methods for the exposure scenarios postulated. In general, the MLE methods tended to have smaller rMSE values than the LPR methods, particularly for the small sample size scenarios. The substitution methods tended to be strongly biased, but in some scenarios had the smaller rMSE values, especially for sample sizes {$<$}20. Surprisingly, the various NP methods were not as robust as expected, performing poorly in the contaminated distribution scenarios for both the 95th percentile and the mean. In conclusion, when using the rMSE rather than bias as the preferred comparison metric, the standard MLE method consistently outperformed the so-called robust variations of the MLE-based and LPR-based methods, as well as the various NP methods, for both the 95th percentile and the mean. When estimating the mean, the standard LPR method tended to outperform the robust LPR-based methods. Whenever bias is the main consideration, the robust MLE-based methods should be considered. The KM method, currently hailed by some as the preferred method for estimating the mean when the lognormal distribution assumption is questioned, did not perform well for either the 95th percentile or mean and is not recommended.},
  langid = {english},
  pmid = {17940277},
  keywords = {Bias,Computer Simulation,{Data Interpretation, Statistical},Humans,Occupational Exposure,Statistics as Topic},
  file = {C:\Users\Zane\Zotero\storage\PDMPWD8A\Hewett and Ganser - 2007 - A comparison of several methods for analyzing cens.pdf}
}

@article{horton1999,
  title = {Maximum Likelihood Analysis of Generalized Linear Models with Missing Covariates},
  author = {Horton, N. J. and Laird, N. M.},
  year = {1999},
  month = mar,
  journal = {Stat. Methods Med. Res.},
  volume = {8},
  number = {1},
  pages = {37--50},
  publisher = {{Arnold, Hodder Headline Plc}},
  address = {{London}},
  issn = {0962-2802},
  doi = {10.1191/096228099673120862},
  urldate = {2023-10-30},
  abstract = {Missing data is a common occurrence in most medical research data collection enterprises. There is an extensive literature concerning missing data, much of which has focused on missing outcomes. Covariates in regression models are often missing, particularly if information is being collected from multiple sources. The method of weights is an implementation of the EM algorithm for general maximum-likelihood analysis of regression models, including generalized linear models (GLMs) with incomplete covariates. In this paper, we will describe the method of weights in detail, illustrate its application with several examples, discuss its advantages and limitations, and review extensions and applications of the method.},
  langid = {english},
  keywords = {CENSORED SURVIVAL-DATA,CHILD PSYCHOPATHOLOGY,EM ALGORITHM,INCOMPLETE DATA,MENTAL-HEALTH,RANDOM ASSUMPTION,REGRESSION-MODELS,VARIABLES,VIOLATION},
  annotation = {Web of Science ID: WOS:000083699900004}
}

@article{huang2009,
  title = {Simultaneous Evaluation of the Magnitude and Breadth of a Left- and Right-Censored Multivariate Response, with Application to {{HIV}} Vaccine Development},
  author = {Huang, Yunda and Gilbert, Peter B. and Montefiori, David C. and Self, Steve G.},
  year = {2009},
  month = feb,
  journal = {STATISTICS IN BIOPHARMACEUTICAL RESEARCH},
  volume = {1},
  number = {1},
  pages = {81--91},
  issn = {1946-6315},
  doi = {10.1198/sbr.2009.0008},
  unique-id = {WOS:000207974700009}
}

@article{irby2021,
  title = {Approaches to Handling Missing or ``Problematic'' Pharmacology Data: {{Pharmacokinetics}}},
  shorttitle = {Approaches to Handling Missing or ``Problematic'' Pharmacology Data},
  author = {Irby, Donald J. and Ibrahim, Mustafa E. and Dauki, Anees M. and Badawi, Mohamed A. and Illamola, S{\'i}lvia M. and Chen, Mingqing and Wang, Yuhuan and Liu, Xiaoxi and Phelps, Mitch A. and Mould, Diane R.},
  year = {2021},
  journal = {CPT: Pharmacometrics \& Systems Pharmacology},
  volume = {10},
  number = {4},
  pages = {291--308},
  issn = {2163-8306},
  doi = {10.1002/psp4.12611},
  urldate = {2023-06-02},
  abstract = {Missing or erroneous information is a common problem in the analysis of pharmacokinetic (PK) data. This may present as missing or inaccurate dose level or dose time, drug concentrations below the analytical limit of quantification, missing sample times, or missing or incorrect covariate information. Several methods to handle problematic data have been evaluated, although no single, broad set of recommendations for commonly occurring errors has been published. In this tutorial, we review the existing literature and present the results of our simulation studies that evaluated common methods to handle known data errors to bridge the remaining gaps and expand on the existing knowledge. This tutorial is intended for any scientist analyzing a PK data set with missing or apparently erroneous data. The approaches described herein may also be useful for the analysis of nonclinical PK data.},
  copyright = {\textcopyright{} 2021 The Authors. CPT: Pharmacometrics \& Systems Pharmacology published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics},
  langid = {english}
}

@article{jaber2021,
  title = {Evaluation of Bias in Weighted Residual Calculations When Handling below the Limit of Quantification Data Using {{Beal}}'s {{M3}} Method},
  author = {Jaber, Mutaz M. and Cheng, Shen and Brundage, Richard C.},
  year = {2021},
  journal = {CPT: Pharmacometrics \& Systems Pharmacology},
  volume = {10},
  number = {4},
  pages = {275--278},
  issn = {2163-8306},
  doi = {10.1002/psp4.12616},
  urldate = {2023-06-02},
  copyright = {\textcopyright{} 2021 The Authors. CPT: Pharmacometrics \& Systems Pharmacology published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.},
  langid = {english},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\J96K9EQR\\Jaber et al. - 2021 - Evaluation of bias in weighted residual calculatio.pdf;C\:\\Users\\Zane\\Zotero\\storage\\5EXT92M3\\psp4.html}
}

@article{jewell1994,
  title = {{{NONPARAMETRIC-ESTIMATION FOR A FORM OF DOUBLY CENSORED-DATA}}, {{WITH APPLICATION TO}} 2 {{PROBLEMS IN AIDS}}},
  author = {JEWELL, {\relax NP} and MALANI, {\relax HM} and VITTINGHOFF, E},
  year = {1994},
  month = mar,
  journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION},
  volume = {89},
  number = {425},
  pages = {7--18},
  issn = {0162-1459},
  unique-id = {WOS:A1994MY54600002}
}

@article{joffe2005,
  title = {Studying Time to Pregnancy by Use of a Retrospective Design},
  author = {Joffe, M. and Key, J. and Best, N. and Keiding, N. and Scheike, T. and Jensen, T. K.},
  year = {2005},
  month = jul,
  journal = {Am. J. Epidemiol.},
  volume = {162},
  number = {2},
  pages = {115--124},
  publisher = {{Oxford Univ Press Inc}},
  address = {{Cary}},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwi172},
  urldate = {2023-10-30},
  abstract = {Biologic fertility can be measured using time to pregnancy (TTP). Retrospective designs, although lacking detailed timed information about behavior and exposure, are useful since they have a well-defined target population, often have good response rates, and are simpler and less expensive to conduct than prospective studies. This paper reviews retrospective TTP studies from a methodological viewpoint and shows how methodological problems can be avoided or minimized by appropriate study design, conduct, and analysis. Sensitivity analyses using data from four European retrospective TTP studies are presented to explore the issues. Although the identified biases tend to have small impacts, the effects are not systematic across studies, and sensitivity analyses are recommended routinely. Planning bias can be checked by comparing propensity to report contraceptive failures in different exposure groups. Medical intervention bias can be avoided by censoring and inclusion of unsuccessful pregnancy attempts. Truncation bias can be a serious problem if unrecognized, but it is avoidable with appropriate study design and/or analysis. Behavior change bias can be minimized by assessing the covariates at the beginning of unprotected intercourse. More complete inference is possible if the study design covers the whole population, not just those who achieve a pregnancy.},
  langid = {english},
  keywords = {data collection,EUROPEAN MULTICENTER,FECUNDABILITY,FECUNDITY,fertility,infertility,INFERTILITY,LONG-TERM RECALL,QUESTIONNAIRE,questionnaires,reproduction,STUDYING HUMAN-FERTILITY,SUBFECUNDITY,TO-PREGNANCY,WAITING TIME},
  annotation = {Web of Science ID: WOS:000230204200002},
  file = {C:\Users\Zane\Zotero\storage\XFJE53ZU\Joffe et al. - 2005 - Studying time to pregnancy by use of a retrospecti.pdf}
}

@article{jusko2012,
  title = {Use of {{Pharmacokinetic Data Below Lower Limit}} of {{Quantitation Values}}},
  author = {Jusko, William J.},
  year = {2012},
  month = sep,
  journal = {Pharm Res},
  volume = {29},
  number = {9},
  pages = {2628--2631},
  issn = {1573-904X},
  doi = {10.1007/s11095-012-0805-6},
  urldate = {2023-06-05},
  langid = {english},
  keywords = {analytical methodology,biomarkers,limit of quantitation,pharmacokinetics},
  file = {C:\Users\Zane\Zotero\storage\QHIHADFS\Jusko - 2012 - Use of Pharmacokinetic Data Below Lower Limit of Q.pdf}
}

@article{kafatos2005,
  title = {Model Selection Methodology for Inter-Laboratory Standardisation of Antibody Titres},
  author = {Kafatos, G and Andrews, N and Nardone, A and {project}, ESEN2},
  year = {2005},
  month = oct,
  journal = {VACCINE},
  volume = {23},
  number = {42},
  pages = {5022--5027},
  issn = {0264-410X},
  doi = {10.1016/j.vaccine.2005.05.030},
  orcid-numbers = {Mossong, Jo\"el/0000-0003-0717-9835 Nardone, Anthony/0000-0003-1138-0937 Green, Manfred S./0000-0002-9753-5612 Cohen, Daniel/0000-0003-2664-2303 Kafatos, George/0000-0001-7871-9216},
  researcherid-numbers = {Mossong, Jo\"el/D-4998-2009},
  unique-id = {WOS:000232265200007}
}

@article{kato2013,
  title = {Bayesian {{Modeling}} of {{Enteric Virus Density}} in {{Wastewater Using Left-Censored Data}}},
  author = {Kato, Tsuyoshi and Miura, Takayuki and Okabe, Satoshi and Sano, Daisuke},
  year = {2013},
  month = dec,
  journal = {Food Environ. Virol.},
  volume = {5},
  number = {4},
  pages = {185--193},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {1867-0334, 1867-0342},
  doi = {10.1007/s12560-013-9125-1},
  urldate = {2023-10-31},
  abstract = {Stochastic models are used to express pathogen density in environmental samples for performing microbial risk assessment with quantitative uncertainty. However, enteric virus density in water often falls below the quantification limit (non-detect) of the analytical methods employed, and it is always difficult to apply stochastic models to a dataset with a substantially high number of non-detects, i.e., left-censored data. We applied a Bayesian model that is able to model both the detected data (detects) and non-detects to simulated left-censored datasets of enteric virus density in wastewater. One hundred paired datasets were generated for each of the 39 combinations of a sample size and the number of detects, in which three sample sizes (12, 24, and 48) and the number of detects from 1 to 12, 24 and 48 were employed. The simulated observation data were assigned to one of two groups, i.e., detects and non-detects, by setting values on the limit of quantification to obtain the assumed number of detects for creating censored datasets. Then, the Bayesian model was applied to the censored datasets, and the estimated mean and standard deviation were compared to the true values by root mean square deviation. The difference between the true distribution and posterior predictive distribution was evaluated by Kullback-Leibler (KL) divergence, and it was found that the estimation accuracy was strongly affected by the number of detects. It is difficult to describe universal criteria to decide which level of accuracy is enough, but eight or more detects are required to accurately estimate the posterior predictive distributions when the sample size is 12, 24, or 48. The posterior predictive distribution of virus removal efficiency with a wastewater treatment unit process was obtained as the log ratio posterior distributions between the posterior predictive distributions of enteric viruses in untreated wastewater and treated wastewater. The KL divergence between the true distribution and posterior predictive distribution of virus removal efficiency also depends on the number of detects, and eight or more detects in a dataset of treated wastewater are required for its accurate estimation.},
  langid = {english},
  keywords = {Bayesian model,CRYPTOSPORIDIUM,Enteric virus density,Left-censored data,MICROBIAL RISK-ASSESSMENT,Non-detects,PARTICLE,Predictive distribution,RECOVERY,SURFACE-WATER,Wastewater},
  annotation = {Web of Science ID: WOS:000326691400001},
  file = {C:\Users\Zane\Zotero\storage\3ZA7IMG9\Kato et al. - 2013 - Bayesian Modeling of Enteric Virus Density in Wast.pdf}
}

@article{keizer2015,
  title = {Incorporation of Concentration Data below the Limit of Quantification in Population Pharmacokinetic Analyses},
  author = {Keizer, Ron J. and Jansen, Robert S. and Rosing, Hilde and Thijssen, Bas and Beijnen, Jos H. and Schellens, Jan H. M. and Huitema, Alwin D. R.},
  year = {2015},
  month = mar,
  journal = {Pharmacol Res Perspect},
  volume = {3},
  number = {2},
  issn = {2052-1707, 2052-1707},
  doi = {10.1002/prp2.131},
  urldate = {2023-08-15},
  abstract = {Handling of data below the lower limit of quantification (LLOQ), below the limit of quantification (BLOQ) in population pharmacokinetic (PopPK) analyses is important for reducing bias and imprecision in parameter estimation. We aimed to evaluate whether using the concentration data below the LLOQ has superior performance over several established methods. The performance of this approach (``All data'') was evaluated and compared to other methods: ``Discard,'' ``LLOQ/2,'' and ``LIKE'' (likelihood-based). An analytical and residual error model was constructed on the basis of in-house analytical method validations and analyses from literature, with additional included variability to account for model misspecification. Simulation analyses were performed for various levels of BLOQ, several structural PopPK models, and additional influences. Performance was evaluated by relative root mean squared error (RMSE), and run success for the various BLOQ approaches. Performance was also evaluated for a real PopPK data set. For all PopPK models and levels of censoring, RMSE values were lowest using ``All data.'' Performance of the ``LIKE'' method was better than the ``LLOQ/2'' or ``Discard'' method. Differences between all methods were small at the lowest level of BLOQ censoring. ``LIKE'' method resulted in low successful minimization ({$<$}50\%) and covariance step success ({$<$}30\%), although estimates were obtained in most runs (\textasciitilde 90\%). For the real PK data set (7.4\% BLOQ), similar parameter estimates were obtained using all methods. Incorporation of BLOQ concentrations showed superior performance in terms of bias and precision over established BLOQ methods, and shown to be feasible in a real PopPK analysis.},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\NNRBYJQL\Keizer et al. - 2015 - Incorporation of concentration data below the limi.pdf}
}

@article{lapidus2011,
  title = {Association between Pandemic and Seasonal Influenza Vaccination and Haemagglutination Antibody Titers against {{A}}/{{H1N1v}}: A National Representative Survey in {{France}}, Nested in the ``{{Cohorts}} for {{Pandemic Influenza}}'' ({{CoPanFlu}} - {{France}})},
  author = {Lapidus, Nathanael and {de Lamballerie}, Xavier and Salez, Nicolas and Moyen, Nanikaly and Ferrari, Pascal and Gougeon, Marie-Lise and Vely, Frederic and {Leruez-Ville}, Marianne and Andreoletti, Laurent and Cauchemez, Simon and Boelle, Pierre-Yves and Vivier, Eric and Abel, Laurent and Schwarzinger, Michael and Setbon, Michel and Legeas, Michele and Le Cann, Pierre and Flahault, Antoine and Carrat, Fabrice},
  year = {2011},
  month = may,
  journal = {INFLUENZA AND OTHER RESPIRATORY VIRUSES},
  volume = {5},
  number = {1},
  pages = {180--183},
  issn = {1750-2640},
  eissn = {1750-2659},
  orcid-numbers = {Schwarzinger, Micha\"el/0000-0002-0573-6856 Vivier, Eric/0000-0001-7022-8287 Lapidus, N./0000-0002-4837-1068 Carrat, Fabrice/0000-0002-8672-7918},
  researcherid-numbers = {Schwarzinger, Micha\"el/F-7367-2011 Vivier, Eric/F-8939-2010 Lapidus, N./AAT-4479-2020 Andreoletti, Laurent/AAQ-2756-2020 V\'ely, Fr\'ed\'eric/HHM-3894-2022 LE CANN, Pierre/K-1854-2015 Carrat, Fabrice/AAU-8480-2020},
  unique-id = {WOS:000289296200062}
}

@article{law1992,
  title = {Effects of Midpoint Imputation on the Analysis of Doubly Censored-Data},
  author = {LAW, {\relax CG} and BROOKMEYER, R},
  year = {1992},
  month = sep,
  journal = {STATISTICS IN MEDICINE},
  volume = {11},
  number = {12},
  pages = {1569--1578},
  issn = {0277-6715},
  doi = {10.1002/sim.4780111204},
  unique-id = {WOS:A1992JX70800003}
}

@article{lee1997,
  title = {Survival Analysis in Public Health Research},
  author = {Lee, E. T. and Go, O. T.},
  year = {1997},
  journal = {Annu. Rev. Public Health},
  volume = {18},
  pages = {105--134},
  publisher = {{Annual Reviews}},
  address = {{Palo Alto}},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.18.1.105},
  urldate = {2023-10-30},
  abstract = {This paper reviews the common statistical techniques employed to analyze survival data in public health research. Due to the presence of censoring, the data are not amenable to the usual method of analysis. The improvement in statistical computing and wide accessibility of personal computers led to the rapid development and popularity of nonparametric over parametric procedures. The former required less stringent conditions. But, if the assumptions for parametric methods hold, the resulting estimates have smaller standard errors and are easier to interpret. Nonparametric techniques include the Kaplan-Meier method for estimating the survival function and the Cox proportional hazards model to identify risk factors and to obtain adjusted risk ratios. In cases where the assumption of proportional hazards is not tenable, the data can be stratified and a model fitted with different baseline functions in each stratum. Parametric modeling such as the accelerated failure time model also may be used. Hazard functions for the exponential, Weibull, gamma, Gompertz, lognormal, and log-logistic distributions are described. Examples from published literature are given to illustrate the various methods. The paper is intended for public health professionals who are interested in survival data analysis.},
  langid = {english},
  keywords = {AIDS,CANCER MORTALITY,censored,CIRRHOSIS,Cox,COX REGRESSION-MODEL,DETERMINISTIC MORTALITY DYNAMICS,DISTRIBUTIONS,Kaplan-Meier,LONGITUDINAL GOMPERTZIAN ANALYSIS,NATURAL CONSEQUENCE,proportional hazards,RANDOMLY CENSORED DATA,survival data,UNITED-STATES},
  annotation = {Web of Science ID: WOS:A1997XA18500005}
}

@article{lee2018,
  title = {Analysis of Generalized Semiparametric Regression Models for Cumulative Incidence Functions with Missing Covariates},
  author = {Lee, Unkyung and Sun, Yanqing and Scheike, Thomas H. and Gilbert, Peter B.},
  year = {2018},
  month = jun,
  journal = {COMPUTATIONAL STATISTICS \& DATA ANALYSIS},
  volume = {122},
  pages = {59--79},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2018.01.003},
  eissn = {1872-7352},
  orcid-numbers = {Scheike, Thomas/0000-0002-2148-4740},
  unique-id = {WOS:000427339400005}
}

@article{leung1997,
  title = {Censoring Issues in Survival Analysis},
  author = {Leung, K. M. and Elashoff, R. M. and Afifi, A. A.},
  year = {1997},
  journal = {Annu. Rev. Public Health},
  volume = {18},
  pages = {83--104},
  publisher = {{Annual Reviews}},
  address = {{Palo Alto}},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.18.1.83},
  urldate = {2023-10-30},
  abstract = {A key characteristic that distinguishes survival analysis from other areas in statistics is that survival data are usually censored. Censoring occurs when incomplete information is available about the survival time of some individuals. We define censoring through some practical examples extracted from the literature in various fields of public health. With few exceptions, the censoring mechanisms in most observational studies are unknown and hence it is necessary to make assumptions about censoring when the common statistical methods are used to analyze censored data. In addition, we present situations in which censoring mechanisms can be ignored. The effects of the censoring assumptions are demonstrated through actual studies.},
  langid = {english},
  keywords = {AIDS,ALUMINUM POTROOM WORKERS,COARSE DATA,COHORT,COMPETING RISKS,FAILURE,ignorability,IGNORABILITY,informative censoring,interval censoring,REGRESSION-ANALYSIS,RESPIRATORY SYMPTOMS,right censoring,survival analysis},
  annotation = {Web of Science ID: WOS:A1997XA18500004}
}

@article{li2008,
  title = {Weighted Likelihood Method for Grouped Survival Data in Case-Cohort Studies with Application to {{HIV}} Vaccine Trials},
  author = {Li, Zhiguo and Gilbert, Peter and Nan, Bin},
  year = {2008},
  month = dec,
  journal = {BIOMETRICS},
  volume = {64},
  number = {4},
  pages = {1247--1255},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2008.00998.x},
  eissn = {1541-0420},
  unique-id = {WOS:000261054500028}
}

@article{lindsey1998,
  title = {Methods for Interval-Censored Data},
  author = {Lindsey, Jane C. and Ryan, Louise M.},
  year = {1998},
  journal = {Statistics in Medicine},
  volume = {17},
  number = {2},
  pages = {219--238},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19980130)17:2<219::AID-SIM735>3.0.CO;2-O},
  urldate = {2023-08-15},
  abstract = {In standard time-to-event or survival analysis, occurrence times of the event of interest are observed exactly or are right-censored, meaning that it is only known that the event occurred after the last observation time. There are numerous methods available for estimating the survival curve and for testing and estimation of the effects of covariates in this context. In some situations, however, the times of the events of interest may only be known to have occurred within an interval of time. In clinical trials, for example, patients are often seen at pre-scheduled visits but the event of interest may occur in between visits. These data are interval-censored. Owing to the lack of well-known statistical methodology and available software, a common ad hoc approach is to assume that the event occurred at the end (or beginning or midpoint) of each interval, and then apply methods for standard time-to-event data. However, this approach can lead to invalid inferences, and in particular will tend to underestimate the standard errors of the estimated parameters. The purpose of this tutorial is to illustrate and compare available methods which correctly treat the data as being interval-censored. It is not meant to be a full review of all existing methods, but only those which are available in standard statistical software, or which can be easily programmed. All approaches will be illustrated on two data sets and compared with methods which ignore the interval-censored nature of the data. We hope this tutorial will allow those familiar with the application of standard survival analysis techniques the option of applying appropriate methods when presented with interval-censored data. \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  langid = {english}
}

@book{little2020,
  title = {Statistical Analysis with Missing Data},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2020},
  series = {Wiley Series in Probability and Statistics},
  edition = {Third edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  isbn = {978-1-118-59601-2 978-0-470-52679-8},
  langid = {english},
  lccn = {QA276},
  keywords = {Mathematical statistics,Missing observations (Statistics),{Problems, exercises, etc}},
  file = {C:\Users\Zane\Zotero\storage\SXYU9AYW\Little and Rubin - 2020 - Statistical analysis with missing data.pdf}
}

@article{liu2013,
  title = {A Two-Step Multiple Imputation for Analysis of Repeated Measures with Left-Censored and Missing Data},
  author = {Liu, G. Frank and Hu, Peter and Mehrotra, Devan V.},
  year = {2013},
  month = may,
  journal = {STATISTICS IN BIOPHARMACEUTICAL RESEARCH},
  volume = {5},
  number = {2},
  pages = {116--125},
  issn = {1946-6315},
  doi = {10.1080/19466315.2013.783503},
  unique-id = {WOS:000322164500004}
}

@article{lumley2021,
  title = {The Duration, Dynamics, and Determinants of Severe Acute Respiratory Syndrome Coronavirus 2 ({{SARS-CoV-2}}) Antibody Responses in Individual Healthcare Workers},
  author = {Lumley, Sheila F. and Wei, Jia and O'Donnell, Denise and Stoesser, Nicole E. and Matthews, Philippa C. and Howarth, Alison and Hatch, Stephanie B. and Marsden, Brian D. and Cox, Stuart and James, Tim and Peck, Liam J. and Ritter, Thomas G. and {de Toledo}, Zoe and Cornall, Richard J. and Jones, E. Yvonne and Stuart, I, David and Screaton, Gavin and Ebner, Daniel and Hoosdally, Sarah and Crook, Derrick W. and Conlon, Christopher P. and Pouwels, Koen B. and Walker, A. Sarah and Peto, Tim E. A. and Walker, Timothy M. and Jeffery, Katie and Eyre, David W. and Grp, Oxford Univ Hosp Staff Testing},
  year = {2021},
  month = aug,
  journal = {CLINICAL INFECTIOUS DISEASES},
  volume = {73},
  number = {3},
  pages = {E699-E709},
  issn = {1058-4838},
  doi = {10.1093/cid/ciab004},
  earlyaccessdate = {MAY 2021},
  eissn = {1537-6591},
  orcid-numbers = {Pouwels, Koen B./0000-0001-7097-8950 Eyre, David W/0000-0001-5095-6367 Stoesser, Nicole/0000-0002-4508-7969 Matthews, Philippa/0000-0002-4036-4269 Ritter, Thomas/0000-0002-3153-342X Rothwell, Evie/0000-0003-4347-0873 Pikoula, Maria/0000-0002-9138-3603 Lumley, Sheila/0000-0001-6825-9324 Skelly, Donal/0000-0002-2426-3097 Hatch, Stephanie/0000-0002-7401-9279 Walker, Timothy/0000-0003-0421-9264 Vorley, Imogen/0000-0003-2922-1862 Wei, Jia/0000-0001-9621-529X Amini, Ali/0000-0002-6837-8881 Shaw, Robert/0000-0003-3449-5876 Mason, Chris/0000-0002-4725-1320 Peck, Liam/0000-0001-9501-1762 De Toledo, Zoe/0000-0001-6992-8040 Shibu, Afrah/0000-0001-7058-357X Holloway, Benjamin/0000-0003-2916-2559 Cutteridge, Joseph/0000-0002-2867-7393 Curtis, Aisling M./0000-0003-3093-6398 Screaton, Gavin/0000-0002-3549-4309 Conway-Jones, Rebecca/0000-0001-5873-6617 Alhussni, Ahmed/0000-0003-1005-8348 Livingstone, Angus/0000-0002-6018-8637 Rodger, Gillian/0000-0001-6051-8985 Ahmed-Firani, Tariq/0000-0003-4541-0583 Fowler, Philip/0000-0003-0912-4483 Crook, Derrick/0000-0002-0590-2850 Pickford, Hayleah/0000-0002-1906-5467 Wilkins, Laura/0000-0003-1562-8688},
  researcherid-numbers = {De Toledo, Zoe/GQQ-1379-2022 Pouwels, Koen B./F-2081-2013 Eyre, David W/P-6887-2016 Stoesser, Nicole/H-3421-2019 Matthews, Philippa/P-7810-2019 Walker, Sarah/HDM-8717-2022 Fowler, Philip/C-5116-2009},
  unique-id = {WOS:000700007300068}
}

@article{mahiane2014,
  title = {Mixture Models for Calibrating the {{BED}} for {{HIV}} Incidence Testing},
  author = {Mahiane, Severin Guy and Fiamma, Agnes and Auvert, Bertran},
  year = {2014},
  month = may,
  journal = {STATISTICS IN MEDICINE},
  volume = {33},
  number = {10},
  pages = {1767--1783},
  issn = {0277-6715},
  doi = {10.1002/sim.6059},
  eissn = {1097-0258},
  unique-id = {WOS:000334028500010}
}

@article{manteiga1994,
  title = {The {{Bootstrap}} - a {{Review}}},
  author = {Manteiga, Wg and Sanchez, Jmp and Romo, J.},
  year = {1994},
  journal = {Comput. Stat.},
  volume = {9},
  number = {3},
  pages = {165--205},
  publisher = {{Physica Verlag Gmbh}},
  address = {{Heidelberg}},
  issn = {0943-4062},
  urldate = {2023-10-30},
  abstract = {The bootstrap, extensively studied for the past fifteen years, has become a powerful tool in different areas of Statistics. In this work, we present the main ideas of bootstrap methodology in several contexts, citing the most relevant contributions and illustrating with examples and simulation studies some interesting aspects.},
  langid = {english},
  keywords = {AUTOREGRESSIVE PROCESSES,BAYESIAN BOOTSTRAP,BAYESIAN METHODS AND PREDICTION ERROR,BOOTSTRAP,CENSORED DATA,CENSORED-DATA,CROSS-VALIDATION,EDGEWORTH EXPANSIONS,ERROR RATE,JACKKNIFE,LINEAR-MODELS,NONPARAMETRIC REGRESSION,PREDICTION RULE,REGRESSION,SIMULTANEOUS CONFIDENCE SETS,SMOOTHING,SYMMETRIZATION},
  annotation = {Web of Science ID: WOS:A1994QC90100001}
}

@article{martinez-florez2013,
  title = {Asymmetric Regression Models with Limited Responses with an Application to Antibody Response to Vaccine},
  author = {{Martinez-Florez}, Guillermo and Bolfarine, Heleno and Gomez, Hector W.},
  year = {2013},
  month = mar,
  journal = {BIOMETRICAL JOURNAL},
  volume = {55},
  number = {2},
  pages = {156--172},
  issn = {0323-3847},
  doi = {10.1002/bimj.201100116},
  eissn = {1521-4036},
  orcid-numbers = {Gomez, Hector W./0000-0003-3726-5507},
  researcherid-numbers = {Bolfarine, Heleno/D-6685-2012},
  unique-id = {WOS:000316086100002}
}

@article{martinez-florez2015,
  title = {Doubly Censored Power-Normal Regression Models with Inflation},
  author = {{Martinez-Florez}, Guillermo and Bolfarine, Heleno and Gomez, Hector W.},
  year = {2015},
  month = jun,
  journal = {TEST},
  volume = {24},
  number = {2},
  pages = {265--286},
  issn = {1133-0686},
  doi = {10.1007/s11749-014-0406-2},
  eissn = {1863-8260},
  orcid-numbers = {Gomez, Hector W./0000-0003-3726-5507},
  researcherid-numbers = {Bolfarine, Heleno/D-6685-2012},
  unique-id = {WOS:000354714100005}
}

@article{martinez-florez2018,
  title = {Censored Bimodal Symmetric-Asymmetric Families},
  author = {{Martinez-Florez}, Guillermo and Bolfarine, Heleno and Gomez, Hector W.},
  year = {2018},
  journal = {STATISTICS AND ITS INTERFACE},
  volume = {11},
  number = {2},
  pages = {237--249},
  issn = {1938-7989},
  doi = {10.4310/SII.2018.v11.n2.a3},
  eissn = {1938-7997},
  researcherid-numbers = {Bolfarine, Heleno/D-6685-2012},
  unique-id = {WOS:000426874600004}
}

@article{martinez-florez2021,
  title = {A Class of Exponentiated Regression Model for Non Negative Censored Data with an Application to Antibody Response to Vaccine},
  author = {{Martinez-Florez}, Guillermo and {Vergara-Cardozo}, Sandra and {Tovar-Falon}, Roger},
  year = {2021},
  month = aug,
  journal = {SYMMETRY-BASEL},
  volume = {13},
  number = {1419},
  doi = {10.3390/sym13081419},
  eissn = {2073-8994},
  orcid-numbers = {Martinez Florez, Guillermo/0000-0001-6441-5377 Tovar-Falon, Roger/0000-0001-5649-532X},
  unique-id = {WOS:000690238000001}
}

@article{martinez-florez2021a,
  title = {The Censored Beta-Skew Alpha-Power Distribution},
  author = {{Martinez-Florez}, Guillermo and {Tovar-Falon}, Roger and {Martinez-Guerra}, Maria},
  year = {2021},
  month = jul,
  journal = {SYMMETRY-BASEL},
  volume = {13},
  number = {1114},
  doi = {10.3390/sym13071114},
  eissn = {2073-8994},
  orcid-numbers = {Martinez Florez, Guillermo/0000-0001-6441-5377 Tovar-Falon, Roger/0000-0001-5649-532X},
  unique-id = {WOS:000677004900001}
}

@article{matsouaka2020,
  title = {Regression with a Right-Censored Predictor Using Inverse Probability Weighting Methods},
  author = {Matsouaka, Roland A. and Atem, Folefac D.},
  year = {2020},
  journal = {Statistics in Medicine},
  volume = {39},
  number = {27},
  pages = {4001--4015},
  issn = {1097-0258},
  doi = {10.1002/sim.8704},
  urldate = {2023-08-15},
  abstract = {In a longitudinal study, measures of key variables might be incomplete or partially recorded due to drop-out, loss to follow-up, or early termination of the study occurring before the advent of the event of interest. In this paper, we focus primarily on the implementation of a regression model with a randomly censored predictor. We examine, particularly, the use of inverse probability weighting methods in a generalized linear model (GLM), when the predictor of interest is right-censored, to adjust for censoring. To improve the performance of the complete-case analysis and prevent selection bias, we consider three different weighting schemes: inverse censoring probability weights, Kaplan-Meier weights, and Cox proportional hazards weights. We use Monte Carlo simulation studies to evaluate and compare the empirical properties of different weighting estimation methods. Finally, we apply these methods to the Framingham Heart Study data as an illustrative example to estimate the relationship between age of onset of a clinically diagnosed cardiovascular event and low-density lipoprotein among cigarette smokers.},
  copyright = {\textcopyright{} 2020 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {censored predictor,Cox proportional hazards model,inverse probability weighting,Kaplan-Meier estimator,regression model},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\CDMNTLY5\\Matsouaka20SiM - Regression with a right‐censored predictor using inverse probability weighting.pdf;C\:\\Users\\Zane\\Zotero\\storage\\H8C3TMMV\\Matsouaka and Atem - 2020 - Regression with a right-censored predictor using i.pdf;C\:\\Users\\Zane\\Zotero\\storage\\VHHRW46A\\sim.html}
}

@article{mcgrory2020,
  title = {Assessment of Groundwater Processes Using Censored Data Analysis Incorporating Non-Detect Chemical, Physical, and Biological Data},
  author = {McGrory, Ellen and Holian, Emma and Morrison, Liam},
  year = {2020},
  month = nov,
  journal = {J. Contam. Hydrol.},
  volume = {235},
  pages = {103706},
  publisher = {{Elsevier}},
  address = {{Amsterdam}},
  issn = {0169-7722, 1873-6009},
  doi = {10.1016/j.jconhyd.2020.103706},
  urldate = {2023-10-31},
  abstract = {In Europe most environmental based water quality research has focused on both nutrient and microbial contamination which can arise from agricultural processes and inadequate wastewater treatment. Recent work in Ireland has linked the presence of arsenic in groundwater at elevated concentrations at national and subnational scales with bedrock lithology serving as a strong predictor variable. Groundwater data was collected as part of an environmental impact assessment for a road construction project and this resulting groundwater geochemistry dataset was used in this present study to assess the geochemical controls of arsenic in natural waters in addition to biological and nutrient contamination. Physiochemical parameters, trace elements, nutrients, organics, and microbiological parameters were collected for every quarter for four years (2004-2008) in 67 wells. Due to differing sampling procedures and limitations in the data, only one quarter (November 2005) was used to understand groundwater geochemistry in greater detail. Multivariate statistical techniques were used to overcome the presence of non-detect data. This is an important consideration as while methods exist for chemical data, methods incorporating biological data are limited. Elevated levels of nitrate in groundwater may arise from the runoff of septic tanks and/or agricultural practices in the area. Both pesticides and polycyclic aromatic hydrocarbons were not detected in any wells signifying no anthropogenic contamination inputs. However, fuel products such as methyl tert-butyl ether were detected and potentially illustrate point source contamination, these were detected in only one well. Geochemical data indicate that elevated arsenic concentrations are present within alkali-oxic groundwaters through the desorption from Fe and Mn oxyhydroxides, i.e. alkali desorption. This study examines of the geochemistry of arsenic in groundwater in Ireland at a local scale. In addition, the multivariate methods used in this study were able to fully integrate both chemical and biological censored data, which may be applied in other regions with similar data.},
  langid = {english},
  keywords = {Arsenic,CHEMISTRY,CONTAMINATION,Environmental monitoring,GEOCHEMISTRY,Microbial contamination,MULTIVARIATE-ANALYSIS,NITRATE,Non-detects,NONDETECTS,SOUTHEAST,SPATIAL-ANALYSIS,SYSTEMS,Trace elements,WATER-QUALITY},
  annotation = {Web of Science ID: WOS:000595169100001}
}

@article{mihalache2023,
  title = {Left-Censored Data and Where to Find Them: {{Current}} Implications in Mycotoxin-Related Risk Assessment, Legislative and Economic Impacts},
  shorttitle = {Left-Censored Data and Where to Find Them},
  author = {Mihalache, Octavian Augustin and Dall'Asta, Chiara},
  year = {2023},
  month = jun,
  journal = {Trends in Food Science \& Technology},
  volume = {136},
  pages = {112--119},
  issn = {0924-2244},
  doi = {10.1016/j.tifs.2023.04.011},
  urldate = {2023-10-30},
  abstract = {Background One of the most challenging steps in chemical risk assessment is the dietary exposure due to the high percentage of left-censored data (LCD). Mycotoxins are the group of chemical contaminants known to show the highest percentage of LCD among chemical contaminants. These data are extremely important especially if they are used for the development of new legislations with permitted maximum limits. Scope and approach In this study, we address the importance of analytical techniques and current methods of dealing with LCD, with the goal of highlighting the effect of LCD on risk assessment, which could have a potential impact on future regulations of mycotoxins in foods and the economic implications of such regulations. Key findings and conclusions Thirteen European Food Safety Authority (EFSA) Scientific Opinions and Reports on the risk of mycotoxins to humans were evaluated. The most used analytical method was based on Liquid Chromatography-Mass Spectrometry (LC-MS). Most of the occurrence data for mycotoxins in EFSA risk assessments over the last decade were condensed between 80 and 100\% LCD. The typical statistical methods to handle LCD, the substitution method and cut-off values approach, frequently lead to the overestimation of the human risk which can ultimately lead to stricter law regulations with economic implication on the agricultural trades. Understanding the obstacles that analysts face and the type of data that risk assessors require will help to close the uncertainties and gaps that currently exist in exposure and risk assessment. Establishing limits of detections (LODs) and limits of quantification (LOQs) while having a clear data purpose can help reduce uncertainties in risk assessment and provide more meaningful exposure evaluations.},
  keywords = {Chemical risk assessment,Dietary exposure,Left-censored data,Limit of detection,Limit of quantification,Mycotoxins},
  file = {C:\Users\Zane\Zotero\storage\EWJT7P6I\S0924224423001218.html}
}

@article{mossong2000,
  title = {Modelling Antibody Response to Measles Vaccine and Subsequent Waning of Immunity in a Low Exposure Population},
  author = {Mossong, J and O'Callaghan, {\relax CJ} and Ratnam, S},
  year = {2000},
  month = oct,
  journal = {VACCINE},
  volume = {19},
  number = {4-5},
  pages = {523--529},
  issn = {0264-410X},
  doi = {10.1016/S0264-410X(00)00175-4},
  orcid-numbers = {Mossong, Jo\"el/0000-0003-0717-9835},
  researcherid-numbers = {Mossong, Jo\"el/D-4998-2009},
  unique-id = {WOS:000089930500022}
}

@article{moulton1995,
  title = {A {{Mixture Model}} with {{Detection Limits}} for {{Regression Analyses}} of {{Antibody Response}} to {{Vaccine}}},
  author = {Moulton, Lawrence H. and Halsey, Neal A.},
  year = {1995},
  journal = {Biometrics},
  volume = {51},
  number = {4},
  eprint = {2533289},
  eprinttype = {jstor},
  pages = {1570--1578},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2533289},
  urldate = {2023-11-06},
  abstract = {Antibody concentration values as determined by quantitative assays often are left-censored due to detection limits or limits established for purposes of specificity. Standard analyses which assume the data arise from a single lognormal response distribution may not be appropriate, when more observations are censored than would be expected under such a model. Interference from maternal antibodies due to vaccination at an early age, for example, could result in a high proportion of nonresponders to vaccine. A mixture model consisting of a censored lognormal distribution and a point distribution located below the detection limit is proposed for such situations. Antibody data from a study of measles vaccine are used to illustrate the utility of this approach and the interpretation of the model parameters.}
}

@article{moulton1996,
  title = {A Mixed Gamma Model for Regression Analyses of Quantitative Assay Data},
  author = {Moulton, {\relax LH} and Halsey, {\relax NA}},
  year = {1996},
  month = aug,
  journal = {VACCINE},
  volume = {14},
  number = {12},
  pages = {1154--1158},
  issn = {0264-410X},
  doi = {10.1016/0264-410X(96)00017-5},
  orcid-numbers = {Moulton, Lawrence/0000-0001-7041-7387},
  unique-id = {WOS:A1996VN90700013}
}

@misc{NADACourse,
  title = {Nondetects {{And Data Analysis Course Registration}}},
  journal = {Practical Stats},
  urldate = {2023-05-31},
  howpublished = {https://practicalstats.com//training//training/nada.php},
  langid = {english},
  file = {C:\Users\Zane\Zotero\storage\76U47PAU\nada.html}
}

@article{nauta2006,
  title = {Eliminating Bias in the Estimation of the Geometric Mean of {{HI}} Titres},
  author = {Nauta, Jozef J. P.},
  year = {2006},
  month = sep,
  journal = {BIOLOGICALS},
  volume = {34},
  number = {3},
  pages = {183--186},
  issn = {1045-1056},
  doi = {10.1016/j.biologicals.2005.09.001},
  unique-id = {WOS:000240233400002}
}

@article{nhat2017,
  title = {Structure of General-Population Antibody Titer Distributions to Influenza {{A}} Virus},
  author = {Nhat, Nguyen Thi Duy and Todd, Stacy and {de Bruin}, Erwin and Thao, Tran Thi Nhu and Vy, Nguyen Ha Thao and Quan, Tran Minh and Vinh, Dao Nguyen and {van Beek}, Janko and Anh, Pham Hong and Lam, Ha Minh and Hung, Nguyen Thanh and Thanh, Nguyen Thi Le and Huy, Huynh Le Anh and Ha, Vo Thi Hong and Baker, Stephen and Thwaites, Guy E. and Lien, Nguyen Thi Nam and Hong, Tran Thi Kim and Farrar, Jeremy and Simmons, Cameron P. and Chau, Nguyen Van Vinh and Koopmans, Marion and Boni, Maciej F.},
  year = {2017},
  month = jul,
  journal = {SCIENTIFIC REPORTS},
  volume = {7},
  number = {6060},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-06177-0},
  orcid-numbers = {Koopmans, Marion/0000-0002-5204-2312 Simmons, Cameron P/0000-0002-9039-7392 Thwaites, Guy E/0000-0002-2858-2087 Ha Minh, Lam/0000-0003-3321-1886 Boni, Maciej/0000-0002-0830-9630 van Beek, Janko/0000-0001-7463-9411 Baker, Stephen/0000-0003-1308-5755 Todd, Stacy/0000-0003-3215-9625},
  researcherid-numbers = {Koopmans, Marion/Y-9170-2019 Simmons, Cameron P/ABA-3896-2021 Thwaites, Guy E/R-4993-2019},
  unique-id = {WOS:000405907800114}
}

@article{nieuwenhuijsen1997,
  title = {Exposure Assessment in Occupational Epidemiology: Measuring Present Exposures with an Example of a Study of Occupational Asthma},
  shorttitle = {Exposure Assessment in Occupational Epidemiology},
  author = {Nieuwenhuijsen, M. J.},
  year = {1997},
  month = nov,
  journal = {Int. Arch. Occup. Environ. Health},
  volume = {70},
  number = {5},
  pages = {295--308},
  publisher = {{Springer}},
  address = {{New York}},
  issn = {0340-0131, 1432-1246},
  doi = {10.1007/s004200050222},
  urldate = {2023-10-30},
  abstract = {The aim of the present paper is to present a comprehensive review of the issues involved in exposure assessment for occupational epidemiology studies and to provide an example. Exposure assessment for occupational epidemiology studies is becoming more quantitatively refined. This paper discusses important issues that need to be taken into account for exposure assessment, with particular reference to occupational asthma. It discusses issues such as survey design, data collection, the effect of measurement error and data interpretation. It presents recently developed methodology to evaluate exposure variability and its effect on the attenuation of risk estimates. It also presents methodology to control for such variability. It uses examples from a recent cohort study of flour millers and bakers. This example shows various characteristics of exposure and demonstrates that various measures of exposure, such as peak and full-shift exposure measurements, are regularly correlated, which has consequences for the analyses of exposure-response relationships. This paper stresses the importance of the recognition and evaluation of exposure variability and its effect on risk estimates and shows that with different exposure grouping schemes, different health risk estimates can be obtained. Quantitative exposure assessment is generally difficult, time-consuming and expensive and many issues need to be taken into account, but it can be rewarding and has become an absolute necessity for many occupational epidemiology studies. Evaluation of components of exposure variance is absolutely necessary. Exposure variability could lead to serious attenuation of risk estimates.},
  langid = {english},
  keywords = {CARBON-BLACK,censored data,components of variance,DIFFERENTIAL MISCLASSIFICATION,dust,EMPLOYEE EXPOSURES,epidemiology,exposure assessment,exposure variability,flour,FLOUR MILLS,ISO RECOMMENDATIONS,LOGNORMAL-DISTRIBUTION,measurement error,measurement strategy,NONDIFFERENTIAL MISCLASSIFICATION,PERSONAL SAMPLER,RESPIRATORY SYMPTOMS,STATISTICAL-METHODS,workers},
  annotation = {Web of Science ID: WOS:A1997YB03300002}
}

@article{onofri2019,
  title = {Analysing Censored Data in Agricultural Research: {{A}} Review with Examples and Software Tips},
  shorttitle = {Analysing Censored Data in Agricultural Research},
  author = {Onofri, Andrea and Piepho, Hans-Peter and Kozak, Marcin},
  year = {2019},
  month = jan,
  journal = {Ann. Appl. Biol.},
  volume = {174},
  number = {1},
  pages = {3--13},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0003-4746, 1744-7348},
  doi = {10.1111/aab.12477},
  urldate = {2023-10-30},
  abstract = {Metric data are usually assessed on a continuous scale with good precision, but sometimes agricultural researchers cannot obtain precise measurements of a variable. Values of such a variable cannot then be expressed as real numbers (e.g., 1.51 or 2.56), but often can be represented by intervals into which the values fall (e.g., from 1 to 2 or from 2 to 3). In this situation, statisticians talk about censoring and censored data, as opposed to missing data, where no information is available at all. Traditionally, in agriculture and biology, three methods have been used to analyse such data: (a) when intervals are narrow, some form of imputation (e.g., mid-point imputation) is used to replace the interval and traditional methods for continuous data are employed (such as analyses of variance [ANOVA] and regression); (b) for time-to-event data, the cumulative proportions of individuals that experienced the event of interest are analysed, instead of the individual observed times-to-event; (c) when intervals are wide and many individuals are collected, non-parametric methods of data analysis are favoured, where counts are considered instead of the individual observed value for each sample element. In this paper, we show that these methods may be suboptimal: The first one does not respect the process of data collection, the second leads to unreliable standard errors (SEs), while the third does not make full use of all the available information. As an alternative, methods of survival analysis for censored data can be useful, leading to reliable inferences and sound hypotheses testing. These methods are illustrated using three examples from plant and crop sciences.},
  langid = {english},
  keywords = {Bayesian model,censored data,interval data,MODELS,R,SAS,SEED-GERMINATION,survival analysis,time-to-event data,TRANSFORMATION},
  annotation = {Web of Science ID: WOS:000454409000002}
}

@article{rabe-hesketh2001,
  title = {Multilevel Models for Censored and Latent Responses},
  author = {{Rabe-Hesketh}, S. and Yang, S. Y. and Pickles, A.},
  year = {2001},
  month = dec,
  journal = {Stat. Methods Med. Res.},
  volume = {10},
  number = {6},
  pages = {409--427},
  publisher = {{SAGE Publications Ltd}},
  address = {{London}},
  issn = {0962-2802, 1477-0334},
  doi = {10.1191/096228001682157634},
  urldate = {2023-10-30},
  abstract = {Multilevel models were originally developed to allow linear regression or ANOVA models to be applied to observations that are not mutually independent. This lack of independence commonly arises due to clustering of the units of observations into 'higher level units' such as patients in hospitals. In linear mixed models, the within-cluster correlations are modelled by including random effects in a linear model. In this paper, we discuss generalizations of linear mixed models suitable for responses subject to systematic and random measurement error and interval censoring. The first example uses data from two cross-sectional surveys of schoolchildren to investigate risk factors for early first experimentation with cigarettes. Here the recalled times of the children's first cigarette are likely to be subject to both systematic and random measurement errors as well as being interval censored. We describe multilevel models for interval censored survival times as special cases of generalized linear mixed models and discuss methods of estimating systematic recall bias. The second example is a longitudinal study of mental health problems of patients nested in clinics. Here the outcome is measured by multiple questionnaires allowing the measurement errors to be modelled within a linear latent growth curve model. The resulting model is a multilevel structural equation model. We briefly discuss such models both as extensions of linear mixed models and as extensions of structural equation models. Several different model structures are examined. An important goal of the paper is to place a number of methods that readers may have considered as being distinct within a single overall modelling framework.},
  langid = {english},
  keywords = {DURATION,INFERENCE,MAXIMUM-LIKELIHOOD-ESTIMATION,TIME},
  annotation = {Web of Science ID: WOS:000172485400004}
}

@article{saulo2021,
  title = {A Class of Asymmetric Regression Models for Left-Censored Data},
  author = {Saulo, Helton and Leao, Jeremias and Nobre, Juvencio and Balakrishnan, Narayanaswamy},
  year = {2021},
  month = feb,
  journal = {BRAZILIAN JOURNAL OF PROBABILITY AND STATISTICS},
  volume = {35},
  number = {1},
  pages = {62--84},
  issn = {0103-0752},
  doi = {10.1214/20-BJPS494},
  orcid-numbers = {Saulo, Helton/0000-0002-4467-8652},
  researcherid-numbers = {Saulo, Helton/R-4670-2016},
  unique-id = {WOS:000616450900006}
}

@article{seaman2001,
  title = {Proportional Hazards Model for Interval-Censored Failure Times and Time-Dependent Covariates: Application to Hazard of {{HIV}} Infection of Injecting Drug Users in Prison},
  author = {Seaman, {\relax SR} and Bird, {\relax SM}},
  year = {2001},
  month = jun,
  journal = {STATISTICS IN MEDICINE},
  volume = {20},
  number = {12},
  pages = {1855--1870},
  issn = {0277-6715},
  doi = {10.1002/sim.809},
  orcid-numbers = {Seaman, Shaun/0000-0003-3726-5937},
  unique-id = {WOS:000169422100010}
}

@article{sherina2020,
  title = {Multiple Imputation and Direct Estimation for {{qPCR}} Data with Non-Detects},
  author = {Sherina, Valeriia and McMurray, Helene R. and Powers, Winslow and Land, Harmut and Love, Tanzy M. T. and McCall, Matthew N.},
  year = {2020},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {545},
  publisher = {{BMC}},
  address = {{London}},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03807-9},
  urldate = {2023-10-31},
  abstract = {Background Quantitative real-time PCR (qPCR) is one of the most widely used methods to measure gene expression. An important aspect of qPCR data that has been largely ignored is the presence of non-detects: reactions failing to exceed the quantification threshold and therefore lacking a measurement of expression. While most current software replaces these non-detects with a value representing the limit of detection, this introduces substantial bias in the estimation of both absolute and differential expression. Single imputation procedures, while an improvement on previously used methods, underestimate residual variance, which can lead to anti-conservative inference. Results We propose to treat non-detects as non-random missing data, model the missing data mechanism, and use this model to impute missing values or obtain direct estimates of model parameters. To account for the uncertainty inherent in the imputation, we propose a multiple imputation procedure, which provides a set of plausible values for each non-detect. We assess the proposed methods via simulation studies and demonstrate the applicability of these methods to three experimental data sets. We compare our methods to mean imputation, single imputation, and a penalized EM algorithm incorporating non-random missingness (PEMM). The developed methods are implemented in the R/Bioconductor package nondetects. Conclusions The statistical methods introduced here reduce discrepancies in gene expression values derived from qPCR experiments in the presence of non-detects, providing increased confidence in downstream analyses.},
  langid = {english},
  keywords = {ALGORITHM,Direct estimation,GENE,Gene expression,Missing not at random (MNAR),Multiple imputation,Non-detects,Quantitative real-time PCR (qPCR)},
  annotation = {Web of Science ID: WOS:000595803800002},
  file = {C:\Users\Zane\Zotero\storage\ZRBNWFUT\Sherina et al. - 2020 - Multiple imputation and direct estimation for qPCR.pdf}
}

@article{shih2002,
  title = {Problems in Dealing with Missing Data and Informative Censoring in Clinical Trials},
  author = {Shih, W. C. J.},
  year = {2002},
  month = jan,
  journal = {Curr. Control Trials Cardivasc. Med.},
  volume = {3},
  pages = {4},
  publisher = {{BMC}},
  address = {{London}},
  issn = {1468-6694},
  doi = {10.1186/1468-6708-3-4},
  urldate = {2023-10-30},
  abstract = {A common problem in clinical trials is the missing data that occurs when patients do not complete the study and drop out without further measurements. Missing data cause the usual statistical analysis of complete or all available data to be subject to bias. There are no universally applicable methods for handling missing data. We recommend the following: (1) Report reasons for dropouts and proportions for each treatment group; (2) Conduct sensitivity analyses to encompass different scenarios of assumptions and discuss consistency or discrepancy among them; (3) Pay attention to minimize the chance of dropouts at the design stage and during trial monitoring; (4) Collect post-dropout data on the primary endpoints, if at all possible; and (5) Consider the dropout event itself an important endpoint in studies with many.},
  langid = {english},
  keywords = {DROP-OUTS,FAILURE,HYPERTENSION,informative missing data,intent to treat,LONGITUDINAL DATA,longitudinal study,missing completely at random},
  annotation = {Web of Science ID: WOS:000179361900004},
  file = {C:\Users\Zane\Zotero\storage\NMPVEWCP\Shih - 2002 - Problems in dealing with missing data and informat.pdf}
}

@article{shoari2018,
  title = {Toward Improved Analysis of Concentration Data: {{Embracing}} Nondetects},
  shorttitle = {Toward Improved Analysis of Concentration Data},
  author = {Shoari, Niloofar and Dube, Jean-Sebastien},
  year = {2018},
  month = mar,
  journal = {Environ. Toxicol. Chem.},
  volume = {37},
  number = {3},
  pages = {643--656},
  publisher = {{Wiley}},
  address = {{Hoboken}},
  issn = {0730-7268, 1552-8618},
  doi = {10.1002/etc.4046},
  urldate = {2023-10-30},
  abstract = {Various statistical tests on concentration data serve to support decision-making regarding characterization and monitoring of contaminated media, assessing exposure to a chemical, and quantifying the associated risks. However, the routine statistical protocols cannot be directly applied because of challenges arising from nondetects or left-censored observations, which are concentration measurements below the detection limit of measuring instruments. Despite the existence of techniques based on survival analysis that can adjust for nondetects, these are seldom taken into account properly. A comprehensive review of the literature showed that managing policies regarding analysis of censored data do not always agree and that guidance from regulatory agencies may be outdated. Therefore, researchers and practitioners commonly resort to the most convenient way of tackling the censored data problem by substituting nondetects with arbitrary constants prior to data analysis, although this is generally regarded as a bias-prone approach. Hoping to improve the interpretation of concentration data, the present article aims to familiarize researchers in different disciplines with the significance of left-censored observations and provides theoretical and computational recommendations (under both frequentist and Bayesian frameworks) for adequate analysis of censored data. In particular, the present article synthesizes key findings from previous research with respect to 3 noteworthy aspects of inferential statistics: estimation of descriptive statistics, hypothesis testing, and regression analysis. Environ Toxicol Chem 2018;37:643-656. (c) 2017 SETAC},
  langid = {english},
  keywords = {CONTAMINATION DATA,COVARIATES SUBJECT,Environmental data analysis,ENVIRONMENTAL DATA SETS,Exposure assessment,Left-censored,LIMIT,Limit of detection,LINEAR-REGRESSION,LOGNORMAL MODEL,MIXED-EFFECTS MODELS,Nondetect,RISK-ASSESSMENT,SUBSTITUTION METHOD,WATER},
  annotation = {Web of Science ID: WOS:000426158000002},
  file = {C:\Users\Zane\Zotero\storage\TXE86BVN\Shoari and Dube - 2018 - Toward improved analysis of concentration data Em.pdf}
}

@article{sinha1997,
  title = {Semiparametric {{Bayesian}} Analysis of Survival Data},
  author = {Sinha, D. and Dey, D. K.},
  year = {1997},
  month = sep,
  journal = {J. Am. Stat. Assoc.},
  volume = {92},
  number = {439},
  pages = {1195--1212},
  publisher = {{Amer Statistical Assoc}},
  address = {{Alexandria}},
  issn = {0162-1459, 1537-274X},
  doi = {10.2307/2965586},
  urldate = {2023-10-30},
  abstract = {This review article investigates the potential of Eayes methods for the analysis of survival data using semiparametric models based on either the hazard or the intensity function. The nonparametric part of every model is assumed to be a realization of a stochastic process. The parametric part, which may include a regression parameter or a parameter quantifying the heterogeneity of a population, is assumed to have a prior distribution with possibly unknown hyperparameters. Careful applications of some recently popular computational tools, including sampling-based algorithms, are used to find posterior estimates of several quantities of interest even when dealing with complex models and unusual data structures. The methodologies developed herein are motivated and aimed at analyzing some common types of survival data from different medical studies; here we focus on univariate survival data in the presence of fixed and time-dependent covariates, multiple event-time data for repeated nonfatal events, and multivariate survival data (subjects are related; e.g., families or litters), each patient with interval-censored infection time and interval-censored disease occurrence time in tandem [e.g., patients with acquired immunodeficiency syndrome (AIDS) and other infectious diseases with long incubation times]. Bayesian exploratory data analysis (EDA) methods and diagnostics for model selection and model assessment are considered for each case. Special attention is given to tests of the parametric modeling assumptions and to censoring.},
  langid = {english},
  keywords = {augmented data,CENSORED-DATA,Cox model,FAILURE TIME DATA,frailty,INCOMPLETE DATA,MCMC algorithm,MONTE-CARLO,NONPARAMETRIC-ESTIMATION,PENALIZED LIKELIHOOD ESTIMATION,posterior estimates,PREDICTIVE APPROACH,prior process,PROPORTIONAL HAZARDS MODEL,REGRESSION-ANALYSIS,STOCHASTIC-PROCESSES},
  annotation = {Web of Science ID: WOS:A1997XU87800042}
}

@book{StanManual,
  title = {Stan {{Modeling Language Users}}' {{Guide}} and {{Reference Manual}}},
  author = {{Stan Development Team}},
  year = {2023},
  edition = {2.33}
}

@article{stepanek2023,
  title = {Machine Learning at the Service of Survival Analysis: {{Predictions}} Using Time-to-Event Decomposition and Classification Applied to a Decrease of Blood Antibodies against {{COVID-19}}},
  author = {Stepanek, Lubomir and Habarta, Filip and Mala, Ivana and Stepanek, Ladislav and Nakladalova, Marie and Borikova, Alena and Marek, Lubos},
  year = {2023},
  month = feb,
  journal = {MATHEMATICS},
  volume = {11},
  number = {819},
  doi = {10.3390/math11040819},
  eissn = {2227-7390},
  orcid-numbers = {Stepanek, Lubomir/0000-0002-8308-4304 Borikova, Alena/0000-0003-0871-4350 Marek, Lubos/0000-0003-4761-1936 Stepanek, Ladislav/0000-0002-7261-1412},
  researcherid-numbers = {\v{S}t\v{e}p\'anek, Ladislav/IWL-9470-2023 Stepanek, Lubomir/AGS-5496-2022},
  unique-id = {WOS:000941645700001}
}

@article{strzalkowska-kominiak2010,
  title = {The Statistical Analysis of Consecutive Survival Data under Serial Dependence},
  author = {{Strzalkowska-Kominiak}, Ewa and Stute, Winfried},
  year = {2010},
  journal = {JOURNAL OF NONPARAMETRIC STATISTICS},
  volume = {22},
  number = {PII 923667883},
  pages = {585--597},
  issn = {1048-5252},
  doi = {10.1080/10485250902971740},
  eissn = {1029-0311},
  orcid-numbers = {Strzalkowska-Kominiak, Ewa/0000-0001-8408-4181},
  researcherid-numbers = {Strzalkowska-Kominiak, Ewa/HIR-9917-2022},
  unique-id = {WOS:000279450800004}
}

@book{sun2022,
  title = {Emerging {{Topics}} in {{Modeling Interval-Censored Survival Data}}},
  editor = {Sun, Jianguo and Chen, Ding-Geng},
  year = {2022},
  series = {{{ICSA Book Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-12366-5},
  urldate = {2023-08-15},
  isbn = {978-3-031-12365-8 978-3-031-12366-5},
  langid = {english},
  keywords = {Case-cohort Studies,Censored Data,Confidence Interval,Hazards Model,Predictive Model,Survival Analysis},
  file = {C:\Users\Zane\Zotero\storage\745HZ3PJ\Sun and Chen - 2022 - Emerging Topics in Modeling Interval-Censored Surv.pdf}
}

@misc{svahn2022,
  title = {Bayesian {{Prediction}} with {{Covariates Subject}} to {{Detection Limits}}},
  author = {Svahn, Caroline and Villani, Mattias},
  year = {2022},
  month = jan,
  number = {arXiv:2201.07874},
  eprint = {2201.07874},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.07874},
  urldate = {2023-06-02},
  abstract = {Missing values in covariates due to censoring by signal interference or lack of sensitivity in the measuring devices are common in industrial problems. We propose a full Bayesian solution to the prediction problem with an efficient Markov Chain Monte Carlo (MCMC) algorithm that updates all the censored covariate values jointly in a random scan Gibbs sampler. We show that the joint updating of missing covariate values can be at least two orders of magnitude more efficient than univariate updating. This increased efficiency is shown to be crucial for quickly learning the missing covariate values and their uncertainty in a real-time decision making context, in particular when there is substantial correlation in the posterior for the missing values. The approach is evaluated on simulated data and on data from the telecom sector. Our results show that the proposed Bayesian imputation gives substantially more accurate predictions than na\textbackslash "ive imputation, and that the use of auxiliary variables in the imputation gives additional predictive power.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\IIVW6ATT\\Svahn and Villani - 2022 - Bayesian Prediction with Covariates Subject to Det.pdf;C\:\\Users\\Zane\\Zotero\\storage\\JVLKSHHC\\2201.html}
}

@article{sweeting2010,
  title = {Estimating the Distribution of the Window Period for Recent {{HIV}} Infections: {{A}} Comparison of Statistical Methods},
  author = {Sweeting, Michael J. and De Angelis, Daniela and Parry, John and Suligoi, Barbara},
  year = {2010},
  month = dec,
  journal = {STATISTICS IN MEDICINE},
  volume = {29},
  number = {30, SI},
  pages = {3194--3202},
  issn = {0277-6715},
  doi = {10.1002/sim.3941},
  orcid-numbers = {Suligoi, Barbara/0000-0001-9623-7659 Sweeting, Michael John/0000-0003-0980-8965 De Angelis, Daniela/0000-0001-6619-6112},
  researcherid-numbers = {Commenges, Daniel/T-6536-2019 SULIGOI, BARBARA/C-6494-2016 Suligoi, Barbara/O-6490-2019 Sweeting, Michael John/U-3917-2019},
  unique-id = {WOS:000285846300012}
}

@article{tan1992,
  title = {Some Results on the Probability-Distributions of Times to Pre-Aids and Aids},
  author = {TAN, {\relax WY} and SINGH, {\relax KP}},
  year = {1992},
  month = nov,
  journal = {MATHEMATICAL AND COMPUTER MODELLING},
  volume = {16},
  number = {11},
  pages = {117--155},
  issn = {0895-7177},
  doi = {10.1016/0895-7177(92)90110-7},
  unique-id = {WOS:A1992JX52900011}
}

@article{tang2018,
  title = {Composite Estimation for Single-Index Models with Responses Subject to Detection Limits},
  author = {Tang, Yanlin and Wang, Huixia Judy and Liang, Hua},
  year = {2018},
  month = sep,
  journal = {SCANDINAVIAN JOURNAL OF STATISTICS},
  volume = {45},
  number = {3},
  pages = {444--464},
  issn = {0303-6898},
  doi = {10.1111/sjos.12307},
  eissn = {1467-9469},
  orcid-numbers = {, Hua/0000-0001-8772-5984 Wang, Huixia/0000-0002-5195-8564},
  unique-id = {WOS:000442500900002}
}

@article{teunis2002,
  title = {Kinetics of the {{IgG}} Antibody Response to Pertussis Toxin after Infection with \textexclamdown i\textquestiondown{{B-pertussis}}\textexclamdown/I\textquestiondown},
  author = {Teunis, {\relax PFM} and {Van der Heijden}, {\relax OG} and De Melker, {\relax HE} and Schellekens, {\relax JFP} and Versteegh, {\relax FGA} and Kretzschmar, {\relax MEE}},
  year = {2002},
  month = dec,
  journal = {EPIDEMIOLOGY AND INFECTION},
  volume = {129},
  number = {3},
  pages = {479--489},
  issn = {0950-2688},
  doi = {10.1017/S0950268802007896},
  orcid-numbers = {Kretzschmar, Mirjam/0000-0002-4394-7697 Versteegh, Florens G.A./0000-0003-4773-5592},
  unique-id = {WOS:000180859100007}
}

@article{tran2021,
  title = {Measuring Association among Censored Antibody Titer Data},
  author = {Tran, Thao M. P. and Abrams, Steven and Aerts, Marc and Maertens, Kirsten and Hens, Niel},
  year = {2021},
  journal = {Statistics in Medicine},
  volume = {40},
  number = {16},
  pages = {3740--3761},
  issn = {1097-0258},
  doi = {10.1002/sim.8995},
  urldate = {2023-10-31},
  abstract = {Censoring due to a limit of detection or limit of quantification happens quite often in many medical studies. Conventional approaches to deal with censoring when analyzing these data include, for example, the substitution method and the complete case (CC) analysis. More recently, maximum likelihood estimation (MLE) has been increasingly used. While the CC analysis and the substitution method usually lead to biased estimates, the MLE approach appears to perform well in many situations. This article proposes an MLE approach to estimate the association between two measurements in the presence of censoring in one or both quantities. The central idea is to use a copula function to join the marginal distributions of the two measurements. In various simulation studies, we show that our approach outperforms existing conventional methods (CC and substitution analyses). In addition, rank-based measures of global association such as Kendall's tau or Spearman's rho can be studied, hence, attention is not only confined to Pearson's product-moment correlation coefficient capturing solely linear association. We have shown in our simulations that our approach is robust to misspecification of the copula function or marginal distributions given a small association. Furthermore, we propose a straightforward MLE method to fit a (multiple) linear regression model in the presence of censoring in a covariate or both the covariate and the response. Given the marginal distribution of the censored covariate, our method outperforms conventional approaches. We also compare and discuss the performance of our method with multiple imputation and missing indicator model approaches.},
  copyright = {\textcopyright{} 2021 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {antibody titers,association,geometric mean concentration,left-censored data,maximum likelihood inference}
}

@article{troxel2002,
  title = {Techniques for Incorporating Longitudinal Measurements into Analyses of Survival Data from Clinical Trials},
  author = {Troxel, A. B.},
  year = {2002},
  month = jun,
  journal = {Stat. Methods Med. Res.},
  volume = {11},
  number = {3},
  pages = {237--245},
  publisher = {{SAGE Publications Ltd}},
  address = {{London}},
  issn = {0962-2802, 1477-0334},
  doi = {10.1191/0962280202sm285ra},
  urldate = {2023-10-30},
  abstract = {This article reviews existing approaches for joint analysis of longitudinal measurements, possibly measured with error or incompletely observed, and event-time data, possibly censored. The models take the form of selection or pattern-mixture models; estimation proceeds via the EM algorithm or Bayesian sampling techniques. The models are compared, their estimation and inferential procedures described, and advantages and disadvantages noted. Examples are discussed from several disease areas, including cancer and AIDS.},
  langid = {english},
  keywords = {MODELS},
  annotation = {Web of Science ID: WOS:000176400400002}
}

@article{turkson2021,
  title = {Handling {{Censoring}} and {{Censored Data}} in {{Survival Analysis}}: {{A Standalone Systematic Literature Review}}},
  shorttitle = {Handling {{Censoring}} and {{Censored Data}} in {{Survival Analysis}}},
  author = {Turkson, Anthony Joe and {Ayiah-Mensah}, Francis and Nimoh, Vivian},
  year = {2021},
  month = sep,
  journal = {Int. J. Math. Math. Sci.},
  volume = {2021},
  pages = {9307475},
  publisher = {{Hindawi Ltd}},
  address = {{London}},
  issn = {0161-1712, 1687-0425},
  doi = {10.1155/2021/9307475},
  urldate = {2023-10-30},
  abstract = {The study recognized the worth of understanding the how's of handling censoring and censored data in survival analysis and the potential biases it might cause if researchers fail to identify and handle the concepts with utmost care. We systematically reviewed the concepts of censoring and how researchers have handled censored data and brought all the ideas under one umbrella. The review was done on articles written in the English language spanning from the late fifties to the present time. We googled through NCBI, PubMed, Google scholar and other websites and identified theories and publications on the research topic. Revelation was that censoring has the potential of biasing results and reducing the statistical power of analyses if not handled with the appropriate techniques it requires. We also found that, besides the four main approaches (complete-data analysis method; imputation approach; dichotomizing the data; the likelihood-based approach) to handling censored data, there were several other innovative approaches to handling censored data. These methods include censored network estimation; conditional mean imputation method; inverse probability of censoring weighting; maximum likelihood estimation; Buckley-Janes least squares algorithm; simple multiple imputation strategy; filter algorithm; Bayesian framework; beta-substitution method; search-and-score-hill-climbing algorithm and constraint-based conditional independence algorithm; frequentist; Markov chain Monte Carlo for imputed data; quantile regression; random effects hierarchical Cox proportional hazards; Lin's Concordance Correlation Coefficient; classical maximum likelihood estimate. We infer that the presence of incomplete information about subjects does not necessarily mean that such information must be discarded, rather they must be incorporated into the study for they might carry certain relevant information that holds the key to the understanding of the research. We anticipate that through this review, researchers will develop a deeper understanding of this concept in survival analysis and select the appropriate statistical procedures for such studies devoid of biases.},
  langid = {english},
  keywords = {REGRESSION-MODELS},
  annotation = {Web of Science ID: WOS:000703290600001},
  file = {C:\Users\Zane\Zotero\storage\22L4J7B7\Turkson et al. - 2021 - Handling Censoring and Censored Data in Survival A.pdf}
}

@article{wang2022,
  title = {{{CondiS}}: {{A}} Conditional Survival Distribution-Based Method for Censored Data Imputation Overcoming the Hurdle in Machine Learning-Based Survival Analysis},
  shorttitle = {{{CondiS}}},
  author = {Wang, Yizhuo and Flowers, Christopher R. and Li, Ziyi and Huang, Xuelin},
  year = {2022},
  month = jul,
  journal = {J Biomed Inform},
  volume = {131},
  pages = {104117},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2022.104117},
  urldate = {2023-10-30},
  abstract = {Data analyses by machine learning (ML) algorithms are gaining popularity in biomedical research. When time-to-event data are of interest, censoring is common and needs to be properly addressed. Most ML methods cannot conveniently and appropriately take the censoring information into consideration, potentially leading to inaccurate or biased results. We aim to develop a general-purpose method for imputing censored survival data, facilitating downstream ML analysis. In this study, we propose a novel method of imputing the survival times for censored observations. The proposal is based on their conditional survival distributions (CondiS) derived from Kaplan-Meier estimators. CondiS can replace censored observations with their best approximations from the statistical model, allowing for direct application of ML methods. When covariates are available, we extend CondiS by incorporating the covariate information through ML modeling (CondiS-X), which further improves the accuracy of the imputed survival time. Compared with existing methods with similar purposes, the proposed methods achieved smaller prediction errors and higher concordance with the underlying true survival times in extensive simulation studies. We also demonstrated the usage and advantages of the proposed methods through two real-world cancer datasets. The major advantage of CondiS is that it allows for the direct application of standard ML techniques for analysis once the censored survival times are imputed. We present a user-friendly R package to implement our method, which is a useful tool for ML-based biomedical research in this era of big data.},
  pmcid = {PMC10099458},
  pmid = {35690348},
  file = {C:\Users\Zane\Zotero\storage\T3D5UX4U\Wang et al. - 2022 - CondiS A conditional survival distribution-based .pdf}
}

@article{white2023,
  title = {Accounting for {{Non-Detects}}: {{Application}} to {{Satellite Ammonia Observations}}},
  shorttitle = {Accounting for {{Non-Detects}}},
  author = {White, Evan and Shephard, Mark W. and {Cady-Pereira}, Karen E. and Kharol, Shailesh K. and Ford, Sean and Dammers, Enrico and Chow, Evan and Thiessen, Nikolai and Tobin, David and Quinn, Greg and O'Brien, Jason and Bash, Jesse},
  year = {2023},
  month = may,
  journal = {Remote Sens.},
  volume = {15},
  number = {10},
  pages = {2610},
  publisher = {{MDPI}},
  address = {{Basel}},
  issn = {2072-4292},
  doi = {10.3390/rs15102610},
  urldate = {2023-10-31},
  abstract = {Presented is a methodology to explicitly identify and account for cloud-free satellite measurements below a sensor's measurement detection level. These low signals can often be found in satellite observations of minor atmospheric species with weak spectral signals (e.g., ammonia (NH3)). Not accounting for these non-detects can high-bias averaged measurements in locations that exhibit conditions below the detection limit of the sensor. The approach taken here is to utilize the information content from the satellite signal to explicitly identify non-detects and then account for them with a consistent approach. The methodology is applied to the CrIS Fast Physical Retrieval (CFPR) ammonia product and results in a more realistic averaged dataset under conditions where there are a significant number of non-detects. These results show that in larger emission source regions (i.e., surface values {$>$} 7.5 ppbv) the non-detects occur less than 5\% of the time and have a relatively small impact (decreases by less than 5\%) on the gridded averaged values (e.g., annual ammonia source regions). However, in regions that have low ammonia concentration amounts (i.e., surface values {$<$} 1 ppbv) the fraction of non-detects can be greater than 70\%, and accounting for these values can decrease annual gridded averaged values by over 50\% and make the distributions closer to what is expected based on surface station observations.},
  langid = {english},
  keywords = {ammonia,CrIS,CRIS,CrIS Ammonia Cloud Detection Algorithm (CACDA),EMISSIONS,NITROGEN,non-detects,RETRIEVAL,Satellite Detection,SOUTHEASTERN AEROSOL RESEARCH},
  annotation = {Web of Science ID: WOS:000996289600001},
  file = {C:\Users\Zane\Zotero\storage\ASPV98LM\White et al. - 2023 - Accounting for Non-Detects Application to Satelli.pdf}
}

@article{wijeysundera2012,
  title = {Techniques for Estimating Health Care Costs with Censored Data: An Overview for the Health Services Researcher},
  shorttitle = {Techniques for Estimating Health Care Costs with Censored Data},
  author = {Wijeysundera, Harindra C. and Wang, Xuesong and Tomlinson, George and Ko, Dennis T. and Krahn, Murray D.},
  year = {2012},
  journal = {ClinicoEconomics and Outcomes Research: CEOR},
  volume = {4},
  pages = {145},
  publisher = {{Dove Press}},
  doi = {10.2147/CEOR.S31552},
  urldate = {2023-10-11},
  abstract = {The aim of this study was to review statistical techniques for estimating the mean population cost using health care cost data that, because of the inability to achieve complete follow-up until death, are right censored. The target audience is health ...},
  langid = {english},
  pmid = {22719214},
  file = {C:\Users\Zane\Zotero\storage\VK6ZYZKK\Wijeysundera et al. - 2012 - Techniques for estimating health care costs with c.pdf}
}

@article{wilber2020,
  title = {Inferring Seasonal Infection Risk at Population and Regional Scales from Serology Samples},
  author = {Wilber, Mark Q. and Webb, Colleen T. and Cunningham, Fred L. and Pedersen, Kerri and Wan, Xiu-Feng and Pepin, Kim M.},
  year = {2020},
  month = jan,
  journal = {ECOLOGY},
  volume = {101},
  number = {e02882},
  issn = {0012-9658},
  doi = {10.1002/ecy.2882},
  earlyaccessdate = {NOV 2019},
  eissn = {1939-9170},
  unique-id = {WOS:000497146000001}
}

@article{winship1992,
  title = {Models for {{Sample Selection Bias}}},
  author = {Winship, C. and Mare, Rd},
  year = {1992},
  journal = {Annu. Rev. Sociol.},
  volume = {18},
  pages = {327--350},
  publisher = {{Annual Reviews Inc}},
  address = {{Palo Alto}},
  issn = {0360-0572},
  doi = {10.1146/annurev.so.18.080192.001551},
  urldate = {2023-10-30},
  abstract = {When observations in social research are selected so that they are not independent of the outcome variables in a study, sample selection leads to biased inferences about social processes. Nonrandom selection is both a source of bias in empirical research and a fundamental aspect of many social processes. This chapter reviews models that attempt to take account of sample selection and their applications in research on labor markets, schooling, legal processes, social mobility, and social networks. Variants of these models apply to outcome variables that are censored or truncated-whether explicitly or incidentally-and include the tobit model, the standard selection model, models for treatment effects in quasi-experimental designs, and endogenous switching models. Heckman's two-stage estimator is the most widely used approach to selection bias, but its results may be sensitive to violations of its assumptions about the way that selection occurs. Recent econometric research has developed a wide variety of promising approaches to selection bias that rely on considerably weaker assumptions. These include a number of semi- and nonparametric approaches to estimating selection models, the use of panel data, and the analyses of bounds of estimates. The large number of available methods and the difficulty of modelling selection indicate that researchers should be explicit about the assumptions behind their methods and should present results that derive from a variety of methods.},
  langid = {english},
  keywords = {CONSISTENT ESTIMATION,EMPIRICAL-MODEL,INCOME,INEQUALITY,LABOR-MARKET,METHODOLOGY,REGRESSION,SAMPLING,SELECTION BIAS,SELF-SELECTION,SEMIPARAMETRIC ESTIMATION,SOCIOLOGICAL DATA,STATISTICS,TRAINING-PROGRAMS},
  annotation = {Web of Science ID: WOS:A1992JJ55100015}
}

@article{wu2018,
  title = {Mixed {{Effects Models}} with {{Censored Covariates}}, with {{Applications}} in {{HIV}}/{{AIDS Studies}}},
  author = {Wu, Lang and Zhang, Hongbin},
  year = {2018},
  journal = {J. Probab. Stat.},
  volume = {2018},
  pages = {1581979},
  publisher = {{Hindawi Ltd}},
  address = {{London}},
  issn = {1687-952X, 1687-9538},
  doi = {10.1155/2018/1581979},
  urldate = {2023-10-30},
  abstract = {Mixed effects models are widely used for modelling clustered data when there are large variations between clusters, since mixed effects models allow for cluster-specific inference. In some longitudinal studies such as HIV/AIDS studies, it is common that some time-varying covariates may be left or right censored due to detection limits, may be missing at times of interest, or may be measured with errors. To address these "incomplete data" problems, a common approach is to model the time-varying covariates based on observed covariate data and then use the fitted model to "predict" the censored or missing or mismeasured covariates. In this article, we provide a review of the common approaches for censored covariates in longitudinal and survival response models and advocate nonlinear mechanistic covariate models if such models are available.},
  langid = {english},
  keywords = {DYNAMICS IN-VIVO,SUBJECT,SURVIVAL-DATA},
  annotation = {Web of Science ID: WOS:000435786900001},
  file = {C:\Users\Zane\Zotero\storage\VYTGLMKH\Wu and Zhang - 2018 - Mixed Effects Models with Censored Covariates, wit.pdf}
}

@article{xu2015,
  title = {A Simple and Powerful Method for the Estimation of Intervention Effects on Serological Endpoints Using Paired Interval-Censored Data},
  author = {Xu, Ying and Lam, K. F. and Ooi, Eng Eong and {Wilder-Smith}, Annelies and Paton, Nicholas I. and Lee, Lawrence S. and Cheung, Yin Bun},
  year = {2015},
  journal = {J Biopharm Stat},
  volume = {25},
  number = {1},
  pages = {124--136},
  issn = {1520-5711},
  doi = {10.1080/10543406.2014.919936},
  abstract = {Clinical trials often use a binary "fold increase" endpoint defined according to the ratio of interval-censored measurement at end-of-study to that at baseline. We propose a simple yet principled analytic approach based on the linear mixed-effects model for interval-censored data for the analysis of such paired measurements. Having estimated the model parameters, the risk ratio can be estimated by explicit composite estimand and the variance is estimated using the delta method. The estimation can be implemented using the existing procedures in popular statistical software. We use antibody data from the Chloroquine for Influenza Prevention Trial for illustration.},
  langid = {english},
  pmid = {24835750},
  keywords = {{Antibodies, Viral},Antiviral Agents,Biomarkers,Chloroquine,Clinical Trials as Topic,Computer Simulation,Fold increase,Humans,{Influenza A Virus, H1N1 Subtype},{Influenza, Human},Interval censored,Linear mixed-effects model,Linear Models,{Models, Statistical},Monte Carlo Method,Odds Ratio,Predictive Value of Tests,Risk ratio,Serologic Tests,Software,Treatment Outcome}
}

@article{xu2015c,
  title = {Estimation of Intervention Effect Using Paired Interval-Censored Data with Clumping below Lower Detection Limit},
  author = {Xu, Ying and Lam, K. F. and Cowling, Benjamin J. and Cheung, Yin Bun},
  year = {2015},
  month = jan,
  journal = {STATISTICS IN MEDICINE},
  volume = {34},
  number = {2},
  pages = {307--316},
  issn = {0277-6715},
  doi = {10.1002/sim.6346},
  eissn = {1097-0258},
  orcid-numbers = {Cowling, Benjamin John/0000-0002-6297-7154},
  researcherid-numbers = {Cowling, Benjamin John/C-4263-2009},
  unique-id = {WOS:000346286700009}
}

@article{xu2016,
  title = {Sample Size Determination for Fold-Increase Endpoints Defined by Paired Interval-Censored Data},
  author = {Xu, Ying and Lam, K. F. and Cheung, Yin Bun},
  year = {2016},
  journal = {JOURNAL OF BIOPHARMACEUTICAL STATISTICS},
  volume = {26},
  number = {5},
  pages = {978--991},
  issn = {1054-3406},
  doi = {10.1080/10543406.2016.1148705},
  eissn = {1520-5711},
  unique-id = {WOS:000384442400013}
}

@article{yang2010,
  title = {Evaluations of {{Bayesian}} and Maximum Likelihood Methods in {{PK}} Models with Below-Quantification-Limit Data},
  author = {Yang, Shuying and Roger, James},
  year = {2010},
  journal = {Pharmaceutical Statistics},
  volume = {9},
  number = {4},
  pages = {313--330},
  issn = {1539-1612},
  doi = {10.1002/pst.400},
  urldate = {2023-06-05},
  abstract = {Pharmacokinetic (PK) data often contain concentration measurements below the quantification limit (BQL). While specific values cannot be assigned to these observations, nevertheless these observed BQL data are informative and generally known to be lower than the lower limit of quantification (LLQ). Setting BQLs as missing data violates the usual missing at random (MAR) assumption applied to the statistical methods, and therefore leads to biased or less precise parameter estimation. By definition, these data lie within the interval [0, LLQ], and can be considered as censored observations. Statistical methods that handle censored data, such as maximum likelihood and Bayesian methods, are thus useful in the modelling of such data sets. The main aim of this work was to investigate the impact of the amount of BQL observations on the bias and precision of parameter estimates in population PK models (non-linear mixed effects models in general) under maximum likelihood method as implemented in SAS and NONMEM, and a Bayesian approach using Markov chain Monte Carlo (MCMC) as applied in WinBUGS. A second aim was to compare these different methods in dealing with BQL or censored data in a practical situation. The evaluation was illustrated by simulation based on a simple PK model, where a number of data sets were simulated from a one-compartment first-order elimination PK model. Several quantification limits were applied to each of the simulated data to generate data sets with certain amounts of BQL data. The average percentage of BQL ranged from 25\% to 75\%. Their influence on the bias and precision of all population PK model parameters such as clearance and volume distribution under each estimation approach was explored and compared. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian method,BQL,censored data,maximum likelihood,population pharmacokinetics model},
  file = {C\:\\Users\\Zane\\Zotero\\storage\\BEU7UXAD\\Yang and Roger - 2010 - Evaluations of Bayesian and maximum likelihood met.pdf;C\:\\Users\\Zane\\Zotero\\storage\\9KQ833M7\\pst.html}
}

@article{yao2022,
  title = {Ensemble Methods for Survival Function Estimation with Time-Varying Covariates},
  author = {Yao, Weichi and Frydman, Halina and Larocque, Denis and Simonoff, Jeffrey S.},
  year = {2022},
  month = nov,
  journal = {Stat. Methods Med. Res.},
  volume = {31},
  number = {11},
  pages = {2217--2236},
  publisher = {{SAGE Publications Ltd}},
  address = {{London}},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/09622802221111549},
  urldate = {2023-10-30},
  abstract = {Survival data with time-varying covariates are common in practice. If relevant, they can improve on the estimation of a survival function. However, the traditional survival forests-conditional inference forest, relative risk forest and random survival forest-have accommodated only time-invariant covariates. We generalize the conditional inference and relative risk forests to allow time-varying covariates. We also propose a general framework for estimation of a survival function in the presence of time-varying covariates. We compare their performance with that of the Cox model and transformation forest, adapted here to accommodate time-varying covariates, through a comprehensive simulation study in which the Kaplan-Meier estimate serves as a benchmark, and performance is compared using the integrated L-2 difference between the true and estimated survival functions. In general, the performance of the two proposed forests substantially improves over the Kaplan-Meier estimate. Taking into account all other factors, under the proportional hazard setting, the best method is always one of the two proposed forests, while under the non-proportional hazard setting, it is the adapted transformation forest. K-fold cross-validation is used as an effective tool to choose between the methods in practice.},
  langid = {english},
  keywords = {dynamic estimation,ERROR,FORESTS,INFERENCE,left-truncated right-censored survival data,MODELS,survival curve estimate,Survival forests,time-varying covariates,TREES},
  annotation = {Web of Science ID: WOS:000830757400001},
  file = {C:\Users\Zane\Zotero\storage\KZSTKZLM\Yao et al. - 2022 - Ensemble methods for survival function estimation .pdf}
}

@article{yu2012,
  title = {Statistical Approaches to Analyzing {{HIV-1}} Neutralizing Antibody Assay Data},
  author = {Yu, Xuesong and Gilbert, Peter B. and Hioe, Catarina E. and {Zolla-Pazner}, Susan and Self, Steven G.},
  year = {2012},
  journal = {STATISTICS IN BIOPHARMACEUTICAL RESEARCH},
  volume = {4},
  number = {1},
  pages = {1--13},
  issn = {1946-6315},
  doi = {10.1080/19466315.2011.633860},
  orcid-numbers = {Zolla-Pazner, Susan/0000-0002-0750-2666},
  researcherid-numbers = {Zolla-Pazner, Susan/S-1864-2019},
  unique-id = {WOS:000307662900001}
}

@incollection{yu2013,
  title = {A {{Review}} of {{Various Models}} for {{Interval-Censored Data}}},
  booktitle = {{{INTERVAL-CENSORED TIME-TO-EVENT DATA}}: {{METHODS AND APPLICATIONS}}},
  author = {Yu, Qiqing and Hsu, Yuting},
  editor = {Chen, D. G. and Sun, J. and Peace, K. E.},
  year = {2013},
  pages = {29--42},
  publisher = {{CRC Press-Taylor \& Francis Group}},
  address = {{Boca Raton}},
  issn = {2154-4298},
  urldate = {2023-10-30},
  isbn = {978-1-4665-0428-8 978-1-4665-0425-7},
  langid = {english},
  keywords = {ASYMPTOTIC PROPERTIES,CONSISTENCY,ESTIMATOR,GMLE,MAXIMUM-LIKELIHOOD},
  annotation = {Web of Science ID: WOS:000369243500003}
}

@article{zelner2019,
  title = {Effects of {{Sequential Influenza A}}({{H1N1}})Pdm09 {{Vaccination}} on {{Antibody Waning}}},
  author = {Zelner, Jon and Petrie, Joshua G and Trangucci, Rob and Martin, Emily T and Monto, Arnold S},
  year = {2019},
  month = jun,
  journal = {The Journal of Infectious Diseases},
  volume = {220},
  number = {1},
  pages = {12--19},
  issn = {0022-1899},
  doi = {10.1093/infdis/jiz055},
  urldate = {2021-06-09},
  abstract = {Antibody waning following influenza vaccination has been repeatedly evaluated, but waning has rarely been studied in the context of longitudinal vaccination history.We developed a Bayesian hierarchical model to assess the effects of sequential influenza A(H1N1)pdm09 vaccination on hemagglutination inhibition antibody boosting and waning in a longitudinal cohort of older children and adults from 2011 to 2016, a period during which the A(H1N1)pdm09 vaccine strain did not change.Antibody measurements from 2057 serum specimens longitudinally collected from 388 individuals were included. Average postvaccination antibody titers were similar across successive vaccinations, but the rate of antibody waning increased with each vaccination. The antibody half-life was estimated to decrease from 32 months (95\% credible interval [CrI], 22\textendash 61 months) following first vaccination to 9 months (95\% CrI, 7\textendash 15 months) following a seventh vaccination.Although the rate of antibody waning increased with successive vaccination, the estimated antibody half-life was longer than a typical influenza season even among the most highly vaccinated. This supports current recommendations for vaccination at the earliest opportunity. Patterns of boosting and waning might be different with the influenza A(H3N2) subtype, which evolves more rapidly and has been most associated with reduced effectiveness following repeat vaccination.}
}

@article{zeng2010,
  title = {Regression Analysis with a Misclassified Covariate from a Current Status Observation Scheme},
  author = {Zeng, Leilei and Cook, Richard J. and Warkentin, Theodore E.},
  year = {2010},
  month = jun,
  journal = {BIOMETRICS},
  volume = {66},
  number = {2},
  pages = {415--425},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2009.01299.x},
  eissn = {1541-0420},
  orcid-numbers = {Cook, Richard/0000-0002-1414-4908 Warkentin, Theodore Earl/0000-0002-8046-7588},
  researcherid-numbers = {Cook, Rebecca/HPG-5035-2023},
  unique-id = {WOS:000278964200010}
}

@article{zhang2004,
  title = {Effect of Percent Non-Detects on Estimation Bias in Censored Distributions},
  author = {Zhang, Z. and Lennox, W. C. and Panu, U. S.},
  year = {2004},
  month = sep,
  journal = {J. Hydrol.},
  volume = {297},
  number = {1-4},
  pages = {74--94},
  publisher = {{Elsevier Science Bv}},
  address = {{Amsterdam}},
  issn = {0022-1694, 1879-2707},
  doi = {10.1016/j.jhydrol.2004.04.017},
  urldate = {2023-10-31},
  abstract = {Uniqueness of the problem surrounding non-detects has been a concern alike to researchers and statisticians dealing with summary statistics while analyzing censored data. To incorporate non-detects in the estimation process, a simple substitution by the MDL (method detection limit) and the maximum likelihood estimation method are routinely implemented as standard methods by US-EPA laboratories. In situations where numerical standards are set at or near the MDL by regulatory agencies, it is prudent and important to closely investigate both the variability in test measurements and the estimation bias, because an inference based on biased estimates could entail significant liabilities. Variability is understood to be not only inevitable but also an inherent and integral part of any chemical analysis or test. In situations where regulatory agencies fail to account for the inherently present variability of test measurements, there is a need for regulated facilities to seek remedial action merely as a consequence of inadequate statistical procedure. This paper utilizes a mathematical approach to derive the bias functions and resulting bias curves are developed to investigate the censored samples from a variety of probability distributions such as normal, log-normal, gamma, and Gumbel distributions. Finally, the bias functions and bias curves are also compared to the results obtained by using Monte Carlo simulations. (C) 2004 Elsevier B.V. All rights reserved.},
  langid = {english},
  keywords = {bias,censoring,dirac function,heaviside function,non-detects,probability distribution function,SAMPLES},
  annotation = {Web of Science ID: WOS:000223292300005}
}

@article{zhang2013,
  title = {Kinetics of Serological Responses in Influenza {{A}}({{H7N9}})-Infected Patients Correlate with Clinical Outcome in {{China}}, 2013},
  author = {Zhang, A. and Huang, Y. and Tian, D. and Lau, E. H. and Wan, Y. and Liu, X. and Dong, Y. and Song, Z. and Zhang, X. and Zhang, J. and Bao, M. and Zhou, M. and Yuan, S. and Sun, J. and Zhu, Z. and Hu, Y. and Chen, L. and Leung, C. Y. and Wu, J. T. and Zhang, Z. and Zhang, Xiaoyan and Peiris, J. S. and Xu, J.},
  year = {2013},
  month = dec,
  journal = {EUROSURVEILLANCE},
  volume = {18},
  number = {20657},
  pages = {38--45},
  issn = {1560-7917},
  doi = {10.2807/1560-7917.ES2013.18.50.20657},
  orcid-numbers = {Lau, Eric/0000-0002-6688-9637 ZHANG, ANLI/0000-0001-7847-6541 Leung, Connie YH/0000-0003-1102-4684},
  researcherid-numbers = {zhu, zhu/JDN-0159-2023 Wu, Joseph/ABD-5880-2021 Leung, Connie Yin Hung/C-4600-2009 Lau, Eric/AAJ-6588-2021 wang, xicheng/IXX-0974-2023 Lau, Eric Ho Yin/C-4487-2009},
  unique-id = {WOS:000328686800007}
}

@article{zhao2018,
  title = {Individual and Population Trajectories of Influenza Antibody Titers over Multiple Seasons in a Tropical Country},
  author = {Zhao, Xiahong and Ning, Yilin and Chen, Mark I-Cheng and Cook, Alex R.},
  year = {2018},
  month = jan,
  journal = {AMERICAN JOURNAL OF EPIDEMIOLOGY},
  volume = {187},
  number = {1},
  pages = {135--143},
  issn = {0002-9262},
  doi = {10.1093/aje/kwx201},
  eissn = {1476-6256},
  orcid-numbers = {Chen, Mark/0000-0001-9369-5830 Chen, Mark/0000-0001-9369-5830 Cook, Alex/0000-0002-6271-5832 Ning, Yilin/0000-0002-6758-4472},
  researcherid-numbers = {Chen, Mark/AGC-5338-2022 Chen, Mark/CAG-2949-2022 Ning, Yilin/AAK-5090-2021 Cook, Alex/S-3576-2019 Cook, Alex/B-5941-2011},
  unique-id = {WOS:000419578200016}
}

@article{zhegnoun2008,
  title = {Dealing with the Non-Detected and Non-Quantified Data. {{The}} Example of the Serum Dioxin Data in the French Dioxin and Incinerators Study},
  author = {Zhegnoun, A and Pascal, M and Fr{\'e}ry, N and Sarter, H and Falq, G and Focant, {\relax JF} and Eppe, G},
  year = {2008},
  journal = {Organohalogen Compounds},
  volume = {69},
  pages = {2288--2291}
}
